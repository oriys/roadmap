import type { QuizQuestion } from "../types";

export const week1: Record<string, QuizQuestion[]> = {
  "w1-1": [
    {
      id: "w1-1-q1",
      question: "在 namespaces(7) 的“Namespace types”表格中，哪组 Flag → 类型/隔离对象的对应关系正确？",
      options: [
        "CLONE_NEWUSER → User（隔离 UID/GID）",
        "CLONE_NEWNET → Mount（隔离挂载点）",
        "CLONE_NEWNS → PID（隔离进程号）",
        "CLONE_NEWPID → Network（隔离网络设备/端口）",
      ],
      answer: 0,
      rationale: "namespaces(7) 表格明确：CLONE_NEWUSER 对应 User namespace，隔离 user/group IDs。",
    },
    {
      id: "w1-1-q2",
      question: "要让当前进程加入一个已存在的 namespace，namespaces(7) 建议使用哪个系统调用？",
      options: [
        "setns(2)：通过指向 /proc/<pid>/ns/* 的文件描述符加入",
        "unshare(2)：加入现有 namespace（不创建新的）",
        "clone(2)：不创建子进程也能加入现有 namespace",
        "ioctl(2)：直接切换到目标 namespace",
      ],
      answer: 0,
      rationale: "namespaces(7) 描述 setns(2) 用于加入已存在的 namespace，目标通过 /proc/pid/ns 下的 fd 指定。",
    },
    {
      id: "w1-1-q3",
      question: "关于创建新 namespace 的权限要求，哪个说法符合 namespaces(7)？",
      options: [
        "大多数类型需要 CAP_SYS_ADMIN；User namespace 是例外（Linux 3.8 起无特权也可创建）",
        "所有 namespace 都必须 root 才能创建",
        "只有 Network namespace 需要 CAP_NET_ADMIN，其它都不需要",
        "创建 User namespace 需要 CAP_SYS_ADMIN，但其它都不需要",
      ],
      answer: 0,
      rationale: "namespaces(7) 指出：除 user namespace 外，clone/unshare 创建新 namespace 在多数情况下需要 CAP_SYS_ADMIN。",
    },
    {
      id: "w1-1-q4",
      question: "/proc/<pid>/ns/pid 与 /proc/<pid>/ns/pid_for_children 的区别是？",
      options: [
        "pid 指向当前进程所属 PID namespace（生命周期内不变）；pid_for_children 影响子进程的 PID namespace，可能因 unshare/setns 改变",
        "两者完全等价，只是名字不同",
        "pid_for_children 记录父进程 PID 值，pid 记录子进程 PID",
        "pid 可通过 setns 改变，pid_for_children 永远不变",
      ],
      answer: 0,
      rationale: "namespaces(7) 说明：/proc/<pid>/ns/pid 对进程来说是固定成员关系；pid_for_children 可因 unshare/setns 改变并影响后续子进程。",
    },
    {
      id: "w1-1-q5",
      question: "namespaces(7) 提到，/proc/<pid>/ns/pid_for_children 可能 readlink 返回空缓冲区的典型原因是？",
      options: [
        "该进程还没有在该 namespace 中创建过子进程",
        "该进程不是 root",
        "系统未启用 cgroup v2",
        "内核不支持 PID namespace",
      ],
      answer: 0,
      rationale: "namespaces(7) 说明 pid_for_children 在第一次在该 namespace 创建子进程前可能是空链接。",
    },
    {
      id: "w1-1-q6",
      question: "当 namespace 内已没有成员进程时，怎样才能“pin”住它使其继续存在？",
      options: [
        "保持 /proc/<pid>/ns/* 的打开 fd 或对其做 bind mount",
        "删除 /proc/<pid>/ns/* 以阻止回收",
        "把 /proc 重新挂载为只读",
        "发送 SIGSTOP 给 init",
      ],
      answer: 0,
      rationale: "namespaces(7) 提到：打开 fd 或 bind mount /proc/<pid>/ns/* 会保持对应 namespace 的引用从而不被回收。",
    },
    {
      id: "w1-1-q7",
      question: "以下哪一项不会导致一个没有成员进程的 namespace 仍然存活（被 pin 住）？",
      options: [
        "存在指向 /proc/<pid>/ns/* 的打开 fd",
        "该 PID namespace 仍被一个 proc 文件系统挂载引用",
        "该 user namespace 拥有（owns）某个非 user namespace",
        "关闭所有终端会话（TTY）",
      ],
      answer: 3,
      rationale: "namespaces(7) 的“Namespace lifetime”列举了多种 pin 因素；TTY 会话状态不是其中之一。",
    },
    {
      id: "w1-1-q8",
      question: "namespaces(7) 中 /proc/sys/user 目录下的 max_*_namespaces 文件主要用于？",
      options: [
        "限制每个用户在当前 user namespace 下可创建的各类 namespace 数量；超限时 clone/unshare 失败并返回 ENOSPC",
        "列出当前系统所有 namespace 的 inode 列表",
        "修改后会立即删除已存在的 namespace",
        "只对 UID 0 生效",
      ],
      answer: 0,
      rationale: "namespaces(7) 说明 /proc/sys/user/max_*_namespaces 是 per-user 限制，触发时 clone/unshare 返回 ENOSPC。",
    },
    {
      id: "w1-1-q9",
      question: "namespaces(7) 提到，读取 /proc/<pid>/ns/* 符号链接的权限主要受什么机制影响？",
      options: [
        "ptrace 的访问模式检查（PTRACE_MODE_READ_FSCREDS）",
        "必须先关闭 SELinux",
        "必须在同一个 mount namespace",
        "仅与文件权限位（rwx）有关",
      ],
      answer: 0,
      rationale: "namespaces(7) 指出 dereference/readlink 这些链接受 ptrace 的访问模式检查约束。",
    },
    {
      id: "w1-1-q10",
      question: "执行 `unshare -Ur` 的典型结果是？",
      options: [
        "创建新的 user namespace（-U），并把调用者映射为新 userns 内的 root（-r）",
        "创建新的 UTS namespace 并修改 hostname",
        "加入一个已存在的 user namespace",
        "创建新的 PID namespace 并让当前进程 PID 变为 1",
      ],
      answer: 0,
      rationale: "unshare(1) 的 -U 创建 user namespace，-r 将调用者映射为新 userns 内的 root（但不等于宿主机 root）。",
    },
    {
      id: "w1-1-q11",
      question: "namespaces(7) 提到的 `/proc/sys/user` 目录从哪个内核版本开始提供？",
      options: ["Linux 3.8", "Linux 4.9", "Linux 5.7", "Linux 6.8"],
      answer: 1,
      rationale: "namespaces(7) 说明：/proc/sys/user 目录自 Linux 4.9 起提供，用于暴露各类 namespace 创建上限。",
    },
    {
      id: "w1-1-q12",
      question: "要限制每个用户可创建的 Mount namespaces 数量，应调整哪个 sysctl 文件？",
      options: ["/proc/sys/user/max_mnt_namespaces", "/proc/sys/user/max_mount_namespaces", "/proc/sys/user/max_fs_namespaces", "/proc/sys/user/max_vfs_namespaces"],
      answer: 0,
      rationale: "namespaces(7) 在 /proc/sys/user 列表中明确给出 max_mnt_namespaces。",
    },
    {
      id: "w1-1-q13",
      question: "关于 `/proc/sys/user/max_*_namespaces` 的“per-user limit”，namespaces(7) 的解释是？",
      options: [
        "同一 user namespace 内，每个用户可分别创建 namespace 直到达到该上限",
        "整个系统共享同一个计数器，达到上限后任何人都不能创建",
        "只有 UID 0 受限制，普通用户不受限制",
        "只对 systemd 管理的进程生效",
      ],
      answer: 0,
      rationale: "namespaces(7) 指出 limits are per-user：同一 user namespace 内，每个用户各自受限。",
    },
    {
      id: "w1-1-q14",
      question: "namespaces(7) 对 `/proc/sys/user/max_*_namespaces` 的说明中，哪项是正确的？",
      options: ["这些限制对所有用户生效，包括 UID 0", "仅对非 root 用户生效", "仅对容器内用户生效", "只对 Network namespace 生效"],
      answer: 0,
      rationale: "namespaces(7) 明确：These limits apply to all users, including UID 0。",
    },
    {
      id: "w1-1-q15",
      question: "namespaces(7) 指出，进程读到的 `/proc/sys/user/max_*_namespaces` 值属于哪个 user namespace？",
      options: [
        "打开该文件的进程所在的 user namespace",
        "永远是 initial user namespace",
        "永远是宿主机的 user namespace",
        "由 PID namespace 决定",
      ],
      answer: 0,
      rationale: "namespaces(7) 说明：这些文件暴露的是 opening process 所在 user namespace 的限制。",
    },
    {
      id: "w1-1-q16",
      question: "当达到 `/proc/sys/user/max_*_namespaces` 上限时，clone(2)/unshare(2) 典型会返回哪个错误？",
      options: ["ENOSPC", "EACCES", "EINVAL", "EEXIST"],
      answer: 0,
      rationale: "namespaces(7) 指出：遇到这些限制时 clone(2) 与 unshare(2) 会以 ENOSPC 失败。",
    },
    {
      id: "w1-1-q17",
      question: "namespaces(7) 提到 initial user namespace 中 `/proc/sys/user/max_*_namespaces` 的默认值通常与什么有关？",
      options: [
        "约等于 `/proc/sys/kernel/threads-max` 的一半",
        "固定为 1024",
        "固定为 MAXINT",
        "等于 CPU 核数",
      ],
      answer: 0,
      rationale: "namespaces(7) 说明：initial user namespace 默认值为 threads-max 的一半。",
    },
    {
      id: "w1-1-q18",
      question: "namespaces(7) 提到 descendant user namespaces 中 `/proc/sys/user/max_*_namespaces` 的默认值通常是？",
      options: ["MAXINT", "threads-max 的一半", "0（默认禁止）", "由 systemd 动态计算"],
      answer: 0,
      rationale: "namespaces(7) 指出：在所有 descendant user namespaces 中，默认值为 MAXINT。",
    },
    {
      id: "w1-1-q19",
      question: "namespaces(7) 的“accounted against ancestor namespaces”含义更接近哪项？",
      options: [
        "创建 namespace 会同时计入祖先 user namespaces 的配额，从而避免用创建新 user namespace 逃逸限制",
        "创建 namespace 只计入当前 user namespace，不会影响祖先",
        "创建 namespace 只计入 PID namespace，与 user namespace 无关",
        "创建 namespace 后会自动提升进程权限",
      ],
      answer: 0,
      rationale: "namespaces(7) 指出创建会计入祖先 user namespaces，确保无法用新 userns 绕过已有限制。",
    },
    {
      id: "w1-1-q20",
      question: "namespaces(7) 中哪种 namespace 被称为 hierarchical（层级型）namespace？",
      options: ["PID 或 User namespace", "Network namespace", "Mount namespace", "UTS namespace"],
      answer: 0,
      rationale: "namespaces(7) 在 Namespace lifetime 部分举例说明 hierarchical namespaces 包括 PID 与 user。",
    },
    {
      id: "w1-1-q21",
      question: "namespaces(7) 提到，以下哪项可能 pin 住一个 IPC namespace（即使其中已无成员进程）？",
      options: ["存在一个 mqueue 文件系统挂载引用该 IPC namespace", "宿主机打开了一个 TCP 监听端口", "有进程在其它 user namespace 中运行", "系统启用了 swap"],
      answer: 0,
      rationale: "namespaces(7) 的 Namespace lifetime 列表包含：IPC namespace 可能被 mqueue 文件系统挂载引用而存活。",
    },
    {
      id: "w1-1-q22",
      question: "namespaces(7) 提到，以下哪项可能 pin 住一个 PID namespace（即使其中已无成员进程）？",
      options: ["存在一个 proc(5) 文件系统挂载引用该 PID namespace", "容器使用 hostNetwork", "禁用 ptrace", "开启 cgroup v2 io 控制器"],
      answer: 0,
      rationale: "namespaces(7) 的 Namespace lifetime 列表包含：PID namespace 可能被对应的 proc(5) 挂载引用。",
    },
    {
      id: "w1-1-q23",
      question: "在 `/proc/<pid>/ns/` 目录中，哪个条目代表“该进程未来创建的子进程所处的 PID namespace”？",
      options: ["pid_for_children", "pid", "net", "mnt"],
      answer: 0,
      rationale: "namespaces(7) 说明：pid_for_children 表示子进程的 PID namespace，可能随 unshare/setns 改变。",
    },
    {
      id: "w1-1-q24",
      question: "`ls -l /proc/$$/ns` 输出里常见的 `net:[4026531969]` 这种格式中，方括号内的数字最接近什么含义？",
      options: ["namespace 的内核对象标识（inode 号）", "该 namespace 内的最大 PID", "网卡数量", "cgroup 层级深度"],
      answer: 0,
      rationale: "namespaces(7) 的示例输出把 /proc/pid/ns/* 展示为 `type:[inode]`，用于区分不同 namespace 对象。",
    },
    {
      id: "w1-1-q25",
      question: "namespaces(7) 总结的 namespaces API 中，除 /proc 文件外包含哪些系统调用？",
      options: ["clone(2) / setns(2) / unshare(2) / ioctl(2)", "fork(2) / execve(2) / wait(2) / kill(2)", "mount(2) / umount(2) / chroot(2) / pivot_root(2)", "socket(2) / bind(2) / connect(2) / accept(2)"],
      answer: 0,
      rationale: "namespaces(7) 在 The namespaces API 部分列出 clone(2)、setns(2)、unshare(2) 与 ioctl_nsfs(2)。",
    },
    {
      id: "w1-1-q26",
      question: "Namespaces compatibility list 文档提示：为何不应把 IPCID/pgid 这类“namespace 内 ID”暴露给不同 namespace 的任务？",
      options: [
        "该 ID 只在获取它的 namespace 内有效，在另一个 namespace 可能指向完全不同的对象",
        "因为会导致内核立刻崩溃",
        "因为这些 ID 一定是全局唯一且可移植",
        "因为暴露 ID 会自动打破 user namespace 隔离",
      ],
      answer: 0,
      rationale: "compatibility-list 指出：IPC/PID 提供的 ID 只在各自 namespace 内有效，跨 namespace 暴露可能引用到别的对象。",
    },
    {
      id: "w1-1-q27",
      question: "Namespaces compatibility list 文档的第 2 点强调：不同 user namespaces 内数值相同的 UID，从 VFS 权限视角应当如何对待？",
      options: [
        "不应被视为同一个用户，访问权限不应等价",
        "应当被视为同一个用户，权限完全等价",
        "只要 GID 不同就没有风险",
        "只影响网络访问，不影响文件权限",
      ],
      answer: 0,
      rationale: "compatibility-list 指出：相同 UID 在不同 user namespaces 中从 VFS 角度不应等价，否则会造成权限语义混乱。",
    },
    {
      id: "w1-1-q28",
      question: "Namespaces compatibility list 文档指出的一个已知问题是？",
      options: [
        "当共享 IPC namespace 时，不同 user namespaces 的用户可能仍能访问相同 IPC 对象（即便 UID 数值相同也不应等价）",
        "共享 Net namespace 会让 CPU 限制失效",
        "共享 UTS namespace 会导致文件系统只读",
        "共享 Mount namespace 会让 PID namespace 被删除",
      ],
      answer: 0,
      rationale: "compatibility-list 的第 2 点说明了这种“不应等价但目前仍可能等价”的问题，涉及共享 IPC namespace 的场景。",
    },
    {
      id: "w1-1-q29",
      question: "《User namespaces and resource control》文档建议：启用 user namespaces 的系统应优先启用哪类 cgroup 来缓解资源滥用风险？",
      options: ["memory control groups（内存控制）", "net_cls（网络分类）", "freezer（冻结）", "rdma（RDMA 限制）"],
      answer: 0,
      rationale: "resource-control 文档建议管理员启用 memory control groups，并配置上限以限制不可信用户的最大内存使用。",
    },
    {
      id: "w1-1-q30",
      question: "《User namespaces and resource control》文档提到的一种配置思路是安装 libcgroup 并配置哪些内容？",
      options: [
        "编辑 /etc/cgrules.conf 与 /etc/cgconfig.conf，并设置 libpam-cgroup",
        "仅编辑 /etc/hosts",
        "仅修改 grub 启动参数",
        "只需重启 docker 服务即可",
      ],
      answer: 0,
      rationale: "resource-control 文档给出示例：可通过 libcgroup 包并配置 /etc/cgrules.conf、/etc/cgconfig.conf 以及 libpam-cgroup。",
    },
  ],
  "w1-2": [
    {
      id: "w1-2-q1",
      question: "在 cgroup v2 中，关于控制器（controller）启用方式，哪个说法正确？",
      options: [
        "默认不启用；可用控制器在 cgroup.controllers；通过向父 cgroup 的 cgroup.subtree_control 写入 +cpu/+memory 等启用",
        "控制器默认对所有 cgroup 启用，无需配置",
        "只能在叶子 cgroup 的 cgroup.controllers 中写入来启用",
        "每个控制器必须挂载到单独层级（每个控制器一棵树）",
      ],
      answer: 0,
      rationale: "cgroup v2 文档说明：控制器可用性在 cgroup.controllers，且需在父级 cgroup.subtree_control 显式启用。",
    },
    {
      id: "w1-2-q2",
      question: "cgroup v2 的 Top-down constraint 指的是？",
      options: [
        "子 cgroup 只能启用父 cgroup 已启用的控制器；若子已启用，父不能关闭",
        "父 cgroup 必须比子 cgroup 资源更少",
        "限制只能自下而上配置",
        "root cgroup 不能启用任何控制器",
      ],
      answer: 0,
      rationale: "cgroup v2 文档的 Top-down constraint：资源分配自上而下，子层级只能继承并细分父层级已启用的控制器。",
    },
    {
      id: "w1-2-q3",
      question: "要在非 root cgroup 的 cgroup.subtree_control 启用 domain controller（如 cpu/memory），必须满足哪条规则？",
      options: [
        "该 cgroup 自身不能直接包含进程（需先把进程迁移到子 cgroup）",
        "该 cgroup 必须至少包含一个进程",
        "只要创建了子 cgroup 就能启用，无需迁移进程",
        "只有 threaded 子树才有此限制",
      ],
      answer: 0,
      rationale: "cgroup v2 的 No Internal Process Constraint：非 root 的 domain cgroup 要启用控制器必须把进程放到叶子节点。",
    },
    {
      id: "w1-2-q4",
      question: "关于 cgroup.procs 的写入行为，哪个说法符合 cgroup v2 文档？",
      options: [
        "一次 write 只能迁移一个进程；写入任一线程的 PID 会迁移整个进程的所有线程",
        "一次 write 可以写多个 PID（多行）批量迁移",
        "写入僵尸进程 PID 也能迁移",
        "只有 root cgroup 才有 cgroup.procs 文件",
      ],
      answer: 0,
      rationale: "cgroup v2 文档说明：写 cgroup.procs 一次只能迁移一个进程，且写入任一线程 PID 会迁移整个进程。",
    },
    {
      id: "w1-2-q5",
      question: "在启用 cgroup v2 的系统中，/proc/<pid>/cgroup 里 v2 的条目格式通常是？",
      options: ["0::<cgroup 路径>", "2:<controller>:<path>", "v2:<path>:<controller>", "只显示 PID，没有路径"],
      answer: 0,
      rationale: "cgroup v2 文档示例：/proc/<pid>/cgroup 的 v2 行格式为 0::<path>。",
    },
    {
      id: "w1-2-q6",
      question: "cpu.max 文件的格式与含义正确的是？",
      options: [
        "两个值：$MAX $PERIOD（单位微秒）；表示每个周期最多消耗 $MAX；$MAX=max 表示不限制",
        "一个值：0/1 表示是否禁用 CPU",
        "三个值：shares/quota/period，且单位毫秒",
        "只对实时调度生效，不影响 CFS",
      ],
      answer: 0,
      rationale: "cgroup v2 文档：cpu.max 以 $MAX $PERIOD（微秒）定义带宽上限，$MAX 可为 max 表示无限制。",
    },
    {
      id: "w1-2-q7",
      question: "关于 memory.high 与 memory.max 的区别，哪个描述符合 cgroup v2 文档？",
      options: [
        "memory.high 是节流/回收压力阈值，超出不会触发 OOM；memory.max 是硬上限，无法回收时会在该 cgroup 内触发 OOM killer",
        "memory.high 是硬上限；memory.max 是软阈值",
        "两者只影响 page cache，不影响匿名内存",
        "memory.max 只在 root cgroup 有效",
      ],
      answer: 0,
      rationale: "cgroup v2 文档：memory.high 超出会 throttling 但不触发 OOM；memory.max 是硬限制，无法回收时会触发 OOM。",
    },
    {
      id: "w1-2-q8",
      question: "Docker 文档中 `--cpus=\"1.5\"` 与 CFS 配额参数的等价关系是？",
      options: [
        "等价于 `--cpu-period=100000 --cpu-quota=150000`（单位微秒）",
        "等价于 `--cpu-period=150000 --cpu-quota=100000`",
        "等价于 `--cpu-shares=1.5`",
        "等价于 `--cpuset-cpus=1,5`",
      ],
      answer: 0,
      rationale: "Docker resource_constraints 文档：--cpus 是对 --cpu-period/--cpu-quota 的更便捷封装，并给出等价示例。",
    },
    {
      id: "w1-2-q9",
      question: "根据 Docker resource_constraints 文档，要防止容器使用 swap，应如何设置？",
      options: ["`--memory` 与 `--memory-swap` 设为相同值", "只设置 `--memory-swap=0`", "只设置 `--memory`，不设置 `--memory-swap`", "设置 `--memory-swap=-1`"],
      answer: 0,
      rationale: "Docker 文档：--memory-swap 表示“内存+swap”的总额度，把它设为与 --memory 相同可防止使用 swap。",
    },
    {
      id: "w1-2-q10",
      question: "Docker 文档为什么强调仅在同时设置了 `-m/--memory` 时才考虑 `--oom-kill-disable`？",
      options: [
        "不设内存上限时禁用 OOM killer 可能导致宿主机内存耗尽并被迫杀宿主进程",
        "因为 `--oom-kill-disable` 会自动提升容器 CPU 配额",
        "因为它会关闭容器网络",
        "因为它会让容器变成只读文件系统",
      ],
      answer: 0,
      rationale: "Docker 文档提示：禁用容器 OOM killer 必须配合内存硬限制，否则可能把宿主机拖到 OOM。",
    },
    {
      id: "w1-2-q11",
      question: "cgroup v2 文档提到：系统启动后最初只有哪个 cgroup 存在？",
      options: ["root cgroup（所有进程初始都属于它）", "kubelet cgroup", "docker cgroup", "system.slice"],
      answer: 0,
      rationale: "cgroup v2 文档说明：Initially, only the root cgroup exists to which all processes belong。",
    },
    {
      id: "w1-2-q12",
      question: "在 cgroup v2 中创建一个子 cgroup 的方式是？",
      options: ["在父 cgroup 目录下创建子目录", "向 cgroup.procs 写入目录名", "执行 `cgroupctl create`", "重启 kubelet 自动生成"],
      answer: 0,
      rationale: "cgroup v2 文档示例：child cgroup 通过创建一个子目录（mkdir）来建立。",
    },
    {
      id: "w1-2-q13",
      question: "读取 `cgroup.procs` 时，cgroup v2 文档提示哪项现象是可能出现的？",
      options: [
        "PID 不保证有序，同一 PID 可能因迁移/回迁或 PID 回收而在读取时重复出现",
        "PID 永远按从小到大排序",
        "每个 PID 只会出现一次且不会复用",
        "只会列出线程 ID（TID），不会列出进程 ID（PID）",
      ],
      answer: 0,
      rationale: "cgroup v2 文档指出：cgroup.procs 的 PID 不排序，且读取过程中可能因迁移/回收出现重复。",
    },
    {
      id: "w1-2-q14",
      question: "向目标 cgroup 的 `cgroup.procs` 写入 PID 迁移进程时，一次 write(2) 最多迁移多少个进程？",
      options: ["1 个进程", "任意多个进程（多行）", "最多 10 个", "取决于 CPU 核数"],
      answer: 0,
      rationale: "cgroup v2 文档：Only one process can be migrated on a single write(2) call。",
    },
    {
      id: "w1-2-q15",
      question: "为什么 zombie 进程无法通过 `cgroup.procs` 迁移到另一个 cgroup？",
      options: ["因为 zombie 不会出现在 cgroup.procs 中，因此无法被移动", "因为 zombie 不能写文件", "因为 zombie 只存在于 root cgroup", "因为 zombie 会自动被 OOM kill"],
      answer: 0,
      rationale: "cgroup v2 文档说明：zombie process does not appear in cgroup.procs and thus can't be moved。",
    },
    {
      id: "w1-2-q16",
      question: "cgroup v2 文档提到：一个没有任何子 cgroup 且没有活进程的 cgroup 如何删除？",
      options: ["直接删除该 cgroup 目录（rmdir）", "向 cgroup.kill 写 1", "kill -9 该目录的 PID", "只能重启主机删除"],
      answer: 0,
      rationale: "cgroup v2 文档：没有 children 或 live processes 的 cgroup 可通过 rmdir 删除。",
    },
    {
      id: "w1-2-q17",
      question: "cgroup v2 文档提到：若一个 cgroup 只关联 zombie 进程（无 live processes），它能否被 rmdir？",
      options: ["可以，被视为 empty（空）cgroup", "不可以，必须等 zombie 消失后才能删除", "不可以，必须先禁用所有 controller", "只有 root cgroup 才允许"],
      answer: 0,
      rationale: "cgroup v2 文档指出：仅与 zombie 相关联的 cgroup 被视为 empty，可以移除。",
    },
    {
      id: "w1-2-q18",
      question: "cgroup v2 文档提到：当一个进程 fork 出子进程时，新进程默认出生在哪个 cgroup？",
      options: ["fork 时父进程所在的 cgroup", "root cgroup", "由子进程自己选择", "由 systemd 自动分配到随机 cgroup"],
      answer: 0,
      rationale: "cgroup v2 文档：child process is born into the cgroup that the forking process belongs to at the time of the operation。",
    },
    {
      id: "w1-2-q19",
      question: "cgroup v2 文档对“controller interface files（如 cpu.*、memory.*）归属”的描述是？",
      options: [
        "由父 cgroup 拥有：在父上启用 controller 会在子 cgroup 中出现对应接口文件",
        "由子 cgroup 自己拥有：启用 controller 只影响本目录，不影响子目录",
        "由内核全局拥有：所有 cgroup 都默认出现全部接口文件",
        "由 systemd 拥有：只有 systemd 才能创建这些文件",
      ],
      answer: 0,
      rationale: "cgroup v2 文档说明：controller 接口文件由父级控制，启用后在子 cgroup 中出现。",
    },
    {
      id: "w1-2-q20",
      question: "在 `cgroup.subtree_control` 一次写入多个操作（如 `+cpu +memory -io`）时，cgroup v2 文档说明其规则是？",
      options: ["要么全部成功要么全部失败", "只要有一个成功就算成功", "会逐条部分成功并忽略失败项", "失败会自动回滚到 root"],
      answer: 0,
      rationale: "cgroup v2 文档：当一次写入包含多个 controller 操作时，either they all succeed or fail。",
    },
    {
      id: "w1-2-q21",
      question: "如果在同一次写入中对同一 controller 指定多次操作（例如 `+cpu -cpu`），cgroup v2 文档指出最终以哪个为准？",
      options: ["最后一个操作生效", "第一个操作生效", "随机生效", "两者会相互抵消并报错"],
      answer: 0,
      rationale: "cgroup v2 文档说明：如果同一 controller 被指定多次，最后一个操作生效。",
    },
    {
      id: "w1-2-q22",
      question: "cgroup v2 文档指出：即使 cpu controller 未启用，`cpu.stat` 也总会报告哪些统计？",
      options: ["usage_usec / user_usec / system_usec", "nr_throttled / throttled_usec", "iops / bps", "rss / cache"],
      answer: 0,
      rationale: "cgroup v2 文档：cpu.stat always reports usage_usec/user_usec/system_usec even if controller disabled。",
    },
    {
      id: "w1-2-q23",
      question: "当 cpu controller 启用后，`cpu.stat` 会额外包含哪组统计（针对 fair-class scheduler）？",
      options: [
        "nr_periods / nr_throttled / throttled_usec（以及 burst 相关字段）",
        "tx_bytes / rx_bytes",
        "oom / oom_kill",
        "read_iops / write_iops",
      ],
      answer: 0,
      rationale: "cgroup v2 文档：启用后 cpu.stat 额外记录 period/throttle/burst 等统计。",
    },
    {
      id: "w1-2-q24",
      question: "cgroup v2 文档中，`cpu.weight` 的默认值与范围（非 idle groups）正确的是？",
      options: ["默认 100，范围 1-10000", "默认 1024，范围 2-65535", "默认 1，范围 1-100", "默认 0，范围 0-1"],
      answer: 0,
      rationale: "cgroup v2 文档：cpu.weight 默认 100，非 idle groups 的范围为 [1, 10000]。",
    },
    {
      id: "w1-2-q25",
      question: "cgroup v2 文档中，`cpu.weight.nice` 的可设置范围是？",
      options: ["-20 到 19", "0 到 100", "1 到 10000", "0 到 1024"],
      answer: 0,
      rationale: "cgroup v2 文档：cpu.weight.nice 使用与 nice(2) 相同的 [-20, 19] 范围。",
    },
    {
      id: "w1-2-q26",
      question: "cgroup v2 文档中，`memory.high` 的默认值通常是？",
      options: ["max", "0", "与 memory.max 相同的硬上限", "与系统总内存相同的字节数"],
      answer: 0,
      rationale: "cgroup v2 文档：memory.high 默认值为 “max”。",
    },
    {
      id: "w1-2-q27",
      question: "Docker resource_constraints 文档对 `--memory-reservation` 的描述哪个正确？",
      options: [
        "它是 soft limit，通常在内存紧张/竞争时生效，且必须小于 `--memory`；不保证容器一定不超过",
        "它是硬上限，超出会立即 OOM kill",
        "它会自动关闭 swap",
        "它会把容器固定在单核 CPU 上",
      ],
      answer: 0,
      rationale: "Docker 文档：--memory-reservation 是软限制，触发条件与 host 低内存/竞争相关，且必须低于 --memory。",
    },
    {
      id: "w1-2-q28",
      question: "Docker resource_constraints 文档中，`--memory-swappiness` 的取值范围是？",
      options: ["0 到 100", "-1 到 1", "0 到 10", "1 到 10000"],
      answer: 0,
      rationale: "Docker 文档：--memory-swappiness 可设为 0-100，用于调整匿名页可被 swap 的比例。",
    },
    {
      id: "w1-2-q29",
      question: "Docker resource_constraints 文档对 `--memory-swap=0` 的解释是？",
      options: ["该设置会被忽略，并被当作未设置（unset）", "表示禁止 swap", "表示只允许 swap 不允许内存", "表示 swap 上限为 0 字节但仍可使用 zswap"],
      answer: 0,
      rationale: "Docker 文档：--memory-swap 设置为 0 时会被忽略，等价于未设置。",
    },
    {
      id: "w1-2-q30",
      question: "Docker resource_constraints 文档对 `--memory-swap=-1` 的解释是？",
      options: ["允许使用无限 swap（上限取决于宿主机可用 swap）", "禁止使用 swap", "将 swap 上限设为 1 字节", "只对 Windows 容器有效"],
      answer: 0,
      rationale: "Docker 文档：--memory-swap 显式设为 -1 时允许 unlimited swap，受宿主机可用 swap 约束。",
    },
  ],
  "w1-3": [
    {
      id: "w1-3-q1",
      question: "Docker 文档中对 OverlayFS 与 overlay2 的关系描述正确的是？",
      options: [
        "OverlayFS 是 Linux 内核文件系统；overlay2 是 Docker 基于 OverlayFS 的存储驱动",
        "overlay2 是内核模块；OverlayFS 是 Docker 的用户态实现",
        "OverlayFS 负责网络隔离；overlay2 负责进程隔离",
        "overlay2 只在 Windows 上可用",
      ],
      answer: 0,
      rationale: "Docker OverlayFS 存储驱动文档明确区分：OverlayFS（内核）与 overlay2（Docker 存储驱动）。",
    },
    {
      id: "w1-3-q2",
      question: "Docker 文档指出，overlay2 在 xfs backing filesystem 上的关键要求是？",
      options: [
        "必须开启 d_type=true（即 ftype=1）",
        "必须关闭 journal",
        "必须使用 4K block size",
        "必须把分区挂载为 noatime 才能工作",
      ],
      answer: 0,
      rationale: "Docker 文档：overlay2 支持 xfs，但要求 Supports d_type 为 true（xfs ftype=1）。",
    },
    {
      id: "w1-3-q3",
      question: "overlay2 驱动原生支持最多叠加多少个 lower（镜像）层？",
      options: ["8", "32", "128", "无限制"],
      answer: 2,
      rationale: "Docker OverlayFS 存储驱动文档指出 overlay2 原生支持最多 128 个 lower layers。",
    },
    {
      id: "w1-3-q4",
      question: "在 overlay 挂载参数中 lowerdir/upperdir/workdir/merged 的角色分别是？",
      options: [
        "lower 只读层，upper 可写层，work 用于内部操作，merged 为最终视图",
        "lower 可写，upper 只读，work 保存日志，merged 为 upper",
        "只有 lower 和 merged 必需，其它可省略",
        "merged 只是缓存目录，可删除",
      ],
      answer: 0,
      rationale: "内核 overlayfs 文档给出典型挂载示例并解释 lowerdir/upperdir/workdir 与 merged 的职责。",
    },
    {
      id: "w1-3-q5",
      question: "内核 overlayfs 文档对 upperdir 与 workdir 的要求是？",
      options: [
        "必须位于同一文件系统且 workdir 为空目录",
        "可以在不同文件系统上",
        "允许与 lowerdir 相同",
        "workdir 可在挂载完成后删除",
      ],
      answer: 0,
      rationale: "overlayfs 文档明确：workdir 需要与 upperdir 位于同一文件系统并且为空目录。",
    },
    {
      id: "w1-3-q6",
      question: "当容器写入一个只存在于 lowerdir（镜像层）的文件时，overlay2/OverlayFS 会发生什么？",
      options: ["触发 copy-up，把文件拷贝到 upperdir 再修改", "直接在 lowerdir 原地写入", "写操作被拒绝", "自动创建新的 lowerdir 层"],
      answer: 0,
      rationale: "Docker overlay2 文档说明写入会先执行 copy_up，把文件从 lower 复制到 upper，再对 upper 写入。",
    },
    {
      id: "w1-3-q7",
      question: "根据 Docker overlay2 文档，容器内删除一个来自 lowerdir 的文件会怎样？",
      options: [
        "在 upperdir 创建 whiteout 文件来隐藏 lowerdir 的同名文件（lowerdir 本身不被删除）",
        "直接删除 lowerdir 的原文件",
        "只删除 upperdir 的文件，因此 lowerdir 文件仍可见",
        "删除操作会失败，因为 lowerdir 只读",
      ],
      answer: 0,
      rationale: "Docker overlay2 文档：删除文件会创建 whiteout 来屏蔽 lower 层的同名文件。",
    },
    {
      id: "w1-3-q8",
      question: "根据 Docker overlay2 文档，容器内删除一个目录（来自 lowerdir）通常会导致？",
      options: [
        "在 upperdir 创建 opaque directory，隐藏 lowerdir 的目录内容",
        "直接递归删除 lowerdir 的目录",
        "把 lowerdir 的目录提升为 upperdir",
        "只影响容器内视图，不会产生任何额外标记",
      ],
      answer: 0,
      rationale: "Docker overlay2 文档：删除目录会创建 opaque directory，效果类似 whiteout，阻止访问 lower 的目录内容。",
    },
    {
      id: "w1-3-q9",
      question: "Docker 概览文档对镜像层（layer）的描述，哪个正确？",
      options: [
        "Dockerfile 每条指令会创建一个镜像层；重建时仅重建变化的层",
        "镜像层是运行时生成，与 Dockerfile 无关",
        "每次构建都会重新生成所有层，无缓存",
        "镜像只有一个层，不分层",
      ],
      answer: 0,
      rationale: "Docker 概览文档说明：每条 Dockerfile 指令形成一层，变更后重建会复用未变化的缓存层。",
    },
    {
      id: "w1-3-q10",
      question: "Docker overlay2 文档提到的 copy-up 可能带来的文件描述符陷阱是？",
      options: [
        "同一文件两个 fd 可能在 copy-up 后分别指向 lower 与 upper 的不同副本；可通过提前触发 copy-up（例如 touch）规避",
        "copy-up 会导致内核崩溃",
        "copy-up 会自动把两个 fd 的写入合并到同一底层文件",
        "copy-up 只发生在删除文件时，与写无关",
      ],
      answer: 0,
      rationale: "Docker overlay2 文档给出示例：copy-up 后两个 fd 可能不再指向同一底层对象，建议提前触发 copy-up 避免混用。",
    },
    {
      id: "w1-3-q11",
      question: "内核 overlayfs 文档指出：lower filesystem 是否必须可写？",
      options: ["不必须，lower 可以是只读，甚至可以是另一个 overlayfs", "必须可写，否则无法挂载", "必须是 tmpfs", "必须与 upper 同一文件系统且可写"],
      answer: 0,
      rationale: "overlayfs 文档说明：lower filesystem 不需要可写，甚至可以是另一个 overlayfs。",
    },
    {
      id: "w1-3-q12",
      question: "内核 overlayfs 文档指出：为什么 NFS 不适合作为常见的 upper filesystem？",
      options: [
        "upper 通常需要支持 trusted.* / user.* xattr 且 readdir 必须提供有效 d_type，NFS 不满足这些要求",
        "NFS 不支持目录",
        "NFS 只能只读",
        "NFS 会强制关闭 copy-up",
      ],
      answer: 0,
      rationale: "overlayfs 文档说明 upper 需要支持 xattr 与 valid d_type，因此 NFS 不适合作为 upper。",
    },
    {
      id: "w1-3-q13",
      question: "overlayfs 文档指出：当同名条目同时出现在 upper 与 lower，且任一层中该条目是非目录时，最终名称解析到哪里？",
      options: ["只解析到 upper 对象，lower 对象被隐藏", "只解析到 lower 对象，upper 被忽略", "两者合并为一个新文件", "解析结果随机"],
      answer: 0,
      rationale: "overlayfs 文档：如果 upper/lower 任一为非目录，同名条目最终只指向 upper，lower 被隐藏。",
    },
    {
      id: "w1-3-q14",
      question: "overlayfs 文档指出：在 merged directory 中，哪些信息通常只会报告 upper directory 的？",
      options: [
        "目录的元数据与扩展属性（metadata/xattr）",
        "文件名列表（readdir 返回的名字）",
        "lowerdir 的 inode 号",
        "所有内容都会合并展示",
      ],
      answer: 0,
      rationale: "overlayfs 文档：目录名列表会合并，但 metadata/xattr 只报告 upper directory 的，lower 的被隐藏。",
    },
    {
      id: "w1-3-q15",
      question: "内核 overlayfs 文档中对“whiteout”的定义正确的是？",
      options: [
        "字符设备 0/0，或带 `trusted.overlay.whiteout` xattr 的零字节普通文件",
        "一个指向 lower 文件的符号链接",
        "一个以 `.wh.` 开头的隐藏文件（无任何额外标记）",
        "只能用 overlay2 驱动才存在，内核 overlayfs 不支持",
      ],
      answer: 0,
      rationale: "overlayfs 文档：whiteout 可是 0/0 字符设备，或零字节文件并设置 trusted.overlay.whiteout xattr。",
    },
    {
      id: "w1-3-q16",
      question: "overlayfs 文档指出：当 merged directory 的 upper 层存在 whiteout 时会发生什么？",
      options: [
        "lower 层同名条目被忽略，whiteout 本身也会对视图隐藏",
        "lower 层同名条目仍然可见",
        "whiteout 会自动同步删除 lower 条目",
        "overlayfs 会拒绝挂载",
      ],
      answer: 0,
      rationale: "overlayfs 文档：upper 的 whiteout 会屏蔽 lower 同名条目，同时 whiteout 自身也隐藏。",
    },
    {
      id: "w1-3-q17",
      question: "内核 overlayfs 文档指出：把目录标记为 opaque 的方式是？",
      options: [
        "在目录上设置 xattr `trusted.overlay.opaque` 为 `y`，从而忽略 lower 中的同名目录",
        "把目录权限改为 000",
        "创建一个 whiteout 文件并放在目录内",
        "把目录重命名为 `.opaque` 前缀",
      ],
      answer: 0,
      rationale: "overlayfs 文档：opaque directory 通过设置 trusted.overlay.opaque=y，屏蔽 lower 同名目录内容。",
    },
    {
      id: "w1-3-q18",
      question: "overlayfs 文档描述 merged directory 的 readdir 合并顺序是？",
      options: ["先读 upper，再读 lower（已存在名字不重复加入）", "先读 lower，再读 upper", "随机", "只读 upper，不读 lower"],
      answer: 0,
      rationale: "overlayfs 文档：readdir 先读取 upper 名单，再读取 lower，避免重复条目。",
    },
    {
      id: "w1-3-q19",
      question: "overlayfs 文档提到 readdir 的缓存行为：哪项描述正确？",
      options: [
        "合并后的 name list 会缓存到 file 结构，目录保持打开时变更可能不可见；seek 到 offset 0 后 readdir 会丢弃并重建缓存",
        "每次 readdir 都会重新扫描 upper+lower，不存在缓存",
        "缓存仅对 lower 生效，upper 永远实时",
        "缓存只在 root 用户访问时生效",
      ],
      answer: 0,
      rationale: "overlayfs 文档：合并 name list 会缓存，目录打开期间变更可能不出现；seekdir(0) 可触发重建。",
    },
    {
      id: "w1-3-q20",
      question: "overlayfs 文档指出：对 lower 或 merged 上的目录执行 rename(2) 默认可能返回什么错误？在什么情况下可避免？",
      options: [
        "默认返回 EXDEV；启用 redirect_dir 后可通过 copy-up 目录并设置 `trusted.overlay.redirect` 来处理",
        "默认返回 EACCES；启用 noatime 后可避免",
        "默认返回 ENOSPC；扩大磁盘即可避免",
        "默认返回 EBUSY；关闭所有进程即可避免",
      ],
      answer: 0,
      rationale: "overlayfs 文档：重命名 lower/merged 目录默认按跨设备处理（EXDEV）；redirect_dir 可改变行为并设置 trusted.overlay.redirect。",
    },
    {
      id: "w1-3-q21",
      question: "overlayfs 文档指出：多个 lowerdir（如 `lowerdir=/l1:/l2:/l3`）的堆叠关系正确的是？",
      options: [
        "从右到左堆叠：右侧更底层，左侧更靠上（/l1 为 top，/l3 为 bottom）",
        "从左到右堆叠：左侧更底层，右侧更靠上",
        "顺序无关",
        "只会使用最右侧目录，其它忽略",
      ],
      answer: 0,
      rationale: "overlayfs 文档：多个 lower 从右到左堆叠，示例中 lower1 为 top、lower3 为 bottom。",
    },
    {
      id: "w1-3-q22",
      question: "overlayfs 文档指出：省略 `upperdir=` 和 `workdir=` 时，overlay 挂载会变成什么模式？",
      options: ["read-only overlay（只读）", "read-write overlay（可写）", "会报错无法挂载", "自动创建临时 upper/work"],
      answer: 0,
      rationale: "overlayfs 文档：省略 upperdir/workdir 时 overlay 为只读挂载。",
    },
    {
      id: "w1-3-q23",
      question: "overlayfs 文档指出：如果 lowerdir 的目录名本身包含冒号（:），在 mount 选项里常见的处理方式是？",
      options: ["用反斜杠转义冒号（\\:）", "用 URL 编码（%3A）", "用双引号包起来即可", "必须改名，不允许包含冒号"],
      answer: 0,
      rationale: "overlayfs 文档：lowerdir 列表用冒号分隔，目录名含冒号时需用单个反斜杠转义。",
    },
    {
      id: "w1-3-q24",
      question: "overlayfs 的 metacopy 特性描述正确的是？",
      options: [
        "对 chmod/chown 等元数据操作仅 copy-up 元数据并打上 `trusted.overlayfs.metacopy`；当文件以写方式打开时再延迟 copy-up 数据并移除该 xattr",
        "启用后写入永远不触发 copy-up",
        "启用后所有文件都会立即完整 copy-up",
        "metacopy 只对目录有效，对文件无效",
      ],
      answer: 0,
      rationale: "overlayfs 文档：metacopy 允许先只 copy-up 元数据（标记 trusted.overlayfs.metacopy），在需要写数据时再延迟复制数据。",
    },
    {
      id: "w1-3-q25",
      question: "Docker overlay2 文档提到：Docker Engine 29.0 及之后默认使用什么作为 image store？",
      options: ["containerd image store", "aufs image store", "etcd image store", "overlay2 image store（必须）"],
      answer: 0,
      rationale: "Docker overlay2 文档备注：Docker Engine 29.0+ 默认使用 containerd image store，overlay2 被新的 snapshotter 方案取代。",
    },
    {
      id: "w1-3-q26",
      question: "Docker overlay2 文档指出：更换 storage driver 的直接影响是什么？",
      options: [
        "本地已有容器与镜像会变得不可访问，应先 docker save 或推送到 registry 以便迁移",
        "只影响网络，不影响镜像与容器",
        "只会让镜像体积变大，不影响可用性",
        "会自动把旧数据无缝迁移到新 driver",
      ],
      answer: 0,
      rationale: "Docker overlay2 文档：Changing the storage driver makes existing containers and images inaccessible，建议先保存/推送镜像。",
    },
    {
      id: "w1-3-q27",
      question: "Docker overlay2 文档强调不要直接操作哪个目录下的文件？",
      options: ["/var/lib/docker", "/etc/docker", "/usr/bin", "/proc"],
      answer: 0,
      rationale: "Docker overlay2 文档 Warning：不要直接修改 /var/lib/docker/ 下的文件与目录，它们由 Docker 管理。",
    },
    {
      id: "w1-3-q28",
      question: "Docker overlay2 文档指出：OverlayFS 的 copy_up 是文件级别（file-level）而非块级别（block-level），意味着？",
      options: [
        "首次写入会复制整个文件，即使只改动很小一部分，可能影响写性能",
        "写入只会复制被修改的块，几乎没有性能开销",
        "copy_up 只发生在删除文件时",
        "copy_up 只对小文件生效，大文件直接原地写",
      ],
      answer: 0,
      rationale: "Docker overlay2 文档：OverlayFS 在 copy_up 时复制整个文件，可能带来明显的写放大与延迟。",
    },
    {
      id: "w1-3-q29",
      question: "Docker overlay2 文档指出：copy_up 触发频率的描述哪个正确？",
      options: ["同一文件只在首次写入时 copy_up，后续写入都在 upper 层进行", "每次写入都会 copy_up", "只有读操作才会 copy_up", "copy_up 只在镜像构建时发生，运行时不会发生"],
      answer: 0,
      rationale: "Docker overlay2 文档：copy_up 只发生在给定文件第一次被写入时，之后写都在 upper 层副本上进行。",
    },
    {
      id: "w1-3-q30",
      question: "Docker overlay2 文档指出：为什么应用需要能处理目录 rename 返回的 EXDEV？",
      options: [
        "因为 directory rename 只有在源与目标都在 top layer 时才允许，否则返回 EXDEV，应用需回退到 copy+unlink 策略",
        "因为 EXDEV 表示磁盘坏了必须立刻重启",
        "因为 EXDEV 表示权限不足，必须 sudo",
        "因为 EXDEV 表示网络不通，需要配置 DNS",
      ],
      answer: 0,
      rationale: "Docker overlay2 文档：rename(2) 在非 top layer 场景会返回 EXDEV，需按文档建议实现 copy/unlink 回退。",
    },
  ],
  "w1-4": [
    {
      id: "w1-4-q1",
      question: "根据 veth(4)，关于 veth 设备对（pair）的描述正确的是？",
      options: [
        "总是成对创建；一端发包另一端立即收到；任一端 down 则链路状态为 down",
        "只能单端存在，不支持成对",
        "必须依赖物理网卡才能创建",
        "只能在同一个 network namespace 内使用",
      ],
      answer: 0,
      rationale: "veth(4) 说明 veth 总是成对出现，任一端 down 会导致 pair 链路状态 down，且一端发送另一端接收。",
    },
    {
      id: "w1-4-q2",
      question: "创建一对 veth 的标准命令是？",
      options: [
        "`ip link add <p1> type veth peer name <p2>`",
        "`ip netns add <p1> peer <p2>`",
        "`brctl addif <p1> <p2>`",
        "`ethtool -S <p1> --create-peer <p2>`",
      ],
      answer: 0,
      rationale: "veth(4) 给出创建 veth pair 的 ip 命令示例：ip link add ... type veth peer name ...。",
    },
    {
      id: "w1-4-q3",
      question: "把现有 veth 的一端移动到指定网络命名空间，常用命令是？",
      options: [
        "`ip link set <p2> netns <ns>`",
        "`ip link set <p2> up <ns>`",
        "`ip netns exec <ns> ip link del <p2>`",
        "`sysctl -w netns=<ns>`",
      ],
      answer: 0,
      rationale: "veth(4) 说明可用 ip link set <ifname> netns <ns> 把一端移动到另一个 network namespace。",
    },
    {
      id: "w1-4-q4",
      question: "关于 Docker bridge 网络，哪项描述符合官方文档？",
      options: [
        "同一 bridge 网络内容器可互通；不同 bridge 网络通常仅能通过发布端口互通",
        "不同 bridge 网络默认完全互通",
        "bridge 网络不支持端口发布（-p）",
        "bridge 网络只能用于 Swarm overlay",
      ],
      answer: 0,
      rationale: "Docker bridge 文档：同一 bridge 内可互通，不同 bridge 网络间通常只能通过 published ports 建立连通。",
    },
    {
      id: "w1-4-q5",
      question: "Docker 文档提到 user-defined bridge 相比默认 bridge（docker0）的关键优势之一是？",
      options: [
        "容器间自动 DNS 解析，可用容器名互访",
        "默认 bridge 才支持 DNS，user-defined 不支持",
        "user-defined bridge 无法隔离不同应用栈",
        "user-defined bridge 不能发布端口",
      ],
      answer: 0,
      rationale: "Docker bridge 文档：user-defined bridge 提供自动 DNS 解析，默认 bridge 只能通过 IP 或 legacy link 互访。",
    },
    {
      id: "w1-4-q6",
      question: "Docker bridge 文档对“运行中容器连接/断开网络”的结论是？",
      options: [
        "运行中容器可 attach/detach 到 user-defined 网络；默认 bridge 想移除需停止并重建容器",
        "任何网络都必须停止容器才能连接/断开",
        "只能连接默认 bridge，user-defined 不支持",
        "attach 网络会自动重启容器",
      ],
      answer: 0,
      rationale: "Docker bridge 文档：user-defined 网络支持运行时 attach/detach；默认 bridge 的变更通常需要重建容器。",
    },
    {
      id: "w1-4-q7",
      question: "端口发布时如果使用 `-p 8080:80` 未指定 host 地址，Docker 默认会？",
      options: [
        "把容器端口发布到所有主机地址（默认绑定地址可配置）",
        "只绑定到 127.0.0.1",
        "只绑定到容器 IP，不会暴露到宿主",
        "默认不发布端口，需额外参数启用",
      ],
      answer: 0,
      rationale: "Docker bridge 文档：未指定 host 地址时，published port 默认绑定到主机所有地址（可通过配置调整默认绑定地址）。",
    },
    {
      id: "w1-4-q8",
      question: "根据 Docker 的 packet-filtering/firewalls 文档，Docker 会为哪些网络模式创建防火墙规则？",
      options: [
        "为 bridge 网络创建；ipvlan/macvlan/host networking 不创建对应规则",
        "所有网络模式都会创建相同规则",
        "只为 host networking 创建规则",
        "只要容器运行就会创建规则，与网络无关",
      ],
      answer: 0,
      rationale: "Docker 文档指出：Docker 为 bridge 网络创建防火墙规则；ipvlan/macvlan/host networking 不创建规则。",
    },
    {
      id: "w1-4-q9",
      question: "在 Linux 上 Docker 关于 IP Forwarding 的默认行为，哪个说法正确？",
      options: [
        "如未开启会启用 net.ipv4.ip_forward 和 net.ipv6.conf.all.forwarding，并把转发默认策略设为 drop（可用 ip-forward-no-drop 关闭）",
        "永远不改 sysctl，也不改转发策略",
        "只启用 IPv6 转发",
        "把转发策略设为 accept 以便充当路由器",
      ],
      answer: 0,
      rationale: "Docker packet-filtering/firewalls 文档说明：Docker 需要 IP 转发，并默认设置转发策略为 drop；可通过 ip-forward-no-drop 调整。",
    },
    {
      id: "w1-4-q10",
      question: "启用 firewalld 且 Docker 的 iptables/ip6tables 选项为 true 时，Docker 会做什么？",
      options: [
        "创建 docker zone（target ACCEPT），把 docker0 等 bridge 接口加入该 zone，并创建 docker-forwarding 转发策略",
        "自动关闭 firewalld",
        "把所有接口加入 public zone",
        "禁用端口发布",
      ],
      answer: 0,
      rationale: "Docker 文档：启用 firewalld 时会创建 docker zone 并添加 Docker bridge 接口，同时配置 docker-forwarding 转发策略。",
    },
    {
      id: "w1-4-q11",
      question: "veth(4) 对 veth 设备的典型用途描述正确的是？",
      options: [
        "可作为 network namespaces 之间的“隧道”，也可用于把一个 namespace 的网络桥接到另一个 namespace 的物理网卡",
        "只能用于同一 namespace 内部通信",
        "只能用于 overlay 网络，bridge 网络不使用",
        "只能用在容器里，宿主机无法创建",
      ],
      answer: 0,
      rationale: "veth(4) 说明：veth 可在 network namespaces 之间通信，也可用于桥接到另一命名空间的物理网络设备。",
    },
    {
      id: "w1-4-q12",
      question: "veth(4) 给出的“创建时就把两端放入不同 netns”的命令形式是？",
      options: [
        "`ip link add <p1> netns <p1-ns> type veth peer <p2> netns <p2-ns>`",
        "`ip netns add <p1-ns> peer <p2-ns>`",
        "`brctl addbr <p1-ns> <p2-ns>`",
        "`ethtool -S <p1> netns <p1-ns>`",
      ],
      answer: 0,
      rationale: "veth(4) 给出示例：创建 veth pair 时可用 netns 参数把两端分别放入不同 network namespace。",
    },
    {
      id: "w1-4-q13",
      question: "根据 veth(4)，如何用 ethtool 找到 veth 接口的 peer 端？",
      options: [
        "先 `ethtool -S <if>` 查看 `peer_ifindex`，再用 `ip link` 通过 ifindex 定位对端",
        "只能通过查看 `/etc/hosts`",
        "必须重启网卡才能看到对端",
        "只能在容器内通过 `docker inspect` 获取",
      ],
      answer: 0,
      rationale: "veth(4) 给出思路：`ethtool -S` 可显示 `peer_ifindex`，再用 `ip link` 查到对应接口。",
    },
    {
      id: "w1-4-q14",
      question: "Docker bridge 文档指出，bridge 网络默认具备哪些行为？",
      options: [
        "同一 bridge 内互通、外部默认不可直接访问、通过 masquerading 出站、支持端口发布",
        "默认所有端口对外网全开放且不做 NAT",
        "默认容器无法访问宿主机",
        "默认 bridge 只支持 IPv6，不支持 IPv4",
      ],
      answer: 0,
      rationale: "Docker bridge 文档概述：允许同网互通、阻止外部直接访问、通过 IP masquerading 出网，并支持 port publishing。",
    },
    {
      id: "w1-4-q15",
      question: "Docker bridge 文档提到 bridge 网络的“masquerading”主要带来的效果是？",
      options: [
        "容器访问外部网络时，外部只看到 Docker host 的 IP 地址",
        "容器获得公网 IP，外部直接看到容器 IP",
        "关闭所有出站流量",
        "让容器共享宿主机网络命名空间",
      ],
      answer: 0,
      rationale: "Docker bridge 文档：bridge 网络 uses masquerading，外部网络只看到 Docker host 的 IP。",
    },
    {
      id: "w1-4-q16",
      question: "Docker bridge 文档指出 bridge 网络的适用范围是？",
      options: [
        "同一 Docker daemon host 内的容器通信；跨主机需要 OS 路由或 overlay network",
        "天然支持跨主机通信，不需要额外网络",
        "只能用于单容器，不能多容器互通",
        "只能用于 Swarm mode",
      ],
      answer: 0,
      rationale: "Docker bridge 文档：bridge networks apply to containers on the same Docker daemon host，跨主机需 OS routing 或 overlay network。",
    },
    {
      id: "w1-4-q17",
      question: "Docker bridge 文档指出默认 bridge 网络（也叫 bridge）何时创建？",
      options: ["启动 Docker 时自动创建，新容器默认连接它", "安装 Linux 内核时创建", "每次 docker run 都创建一个新的默认 bridge", "只有执行 docker network create 才会创建"],
      answer: 0,
      rationale: "Docker bridge 文档：启动 Docker 后会自动创建默认 bridge 网络，新启动容器默认连接它。",
    },
    {
      id: "w1-4-q18",
      question: "在默认 bridge 网络上，容器想通过名字互访通常需要什么（文档称为 legacy）？",
      options: ["`--link`（legacy）或改用 user-defined bridge 的自动 DNS", "必须手工配置 iptables DNAT", "必须把容器切到 host network", "必须关闭 IP forwarding"],
      answer: 0,
      rationale: "Docker bridge 文档：默认 bridge 只能用 IP 互访，除非使用 legacy 的 --link；user-defined bridge 则可直接 DNS 解析。",
    },
    {
      id: "w1-4-q19",
      question: "Docker bridge 文档为什么不推荐用手工修改容器 `/etc/hosts` 来实现名字互访？",
      options: ["会带来难以调试的问题（difficult to debug）", "因为 /etc/hosts 是只读的", "因为会导致内核 panic", "因为会强制容器无法出网"],
      answer: 0,
      rationale: "Docker bridge 文档：手工操作 /etc/hosts 会产生难以排查的问题，推荐使用 user-defined network 的 DNS。",
    },
    {
      id: "w1-4-q20",
      question: "Docker bridge 文档说 user-defined bridge 比默认 bridge 隔离更好，主要原因是？",
      options: [
        "默认 bridge 会把未指定 --network 的容器都挂上去，导致不相关服务也能互通；user-defined 网络提供 scoped 隔离域",
        "user-defined bridge 会关闭所有网络访问",
        "默认 bridge 只支持 IPv4",
        "user-defined bridge 不能发布端口",
      ],
      answer: 0,
      rationale: "Docker bridge 文档：默认 bridge 容易把不相关容器放在同一网络导致互通风险；user-defined 网络提供更好的隔离。",
    },
    {
      id: "w1-4-q21",
      question: "Docker bridge 文档对 `--link` 的定位是？",
      options: ["legacy（不推荐的新项目依赖它）", "必须开启的安全特性", "Kubernetes 专用参数", "只能在 Windows 使用"],
      answer: 0,
      rationale: "Docker bridge 文档明确把 --link 标记为 legacy，并推荐使用 user-defined bridge 的 DNS。",
    },
    {
      id: "w1-4-q22",
      question: "Docker bridge 文档提到在 user-defined networks 上不再通过 `--link` 共享环境变量，但推荐用哪些方式替代？",
      options: ["使用 Docker volume / docker-compose / Swarm secrets & configs 等", "只修改 /etc/hosts", "把环境变量写进镜像层并强制重建", "只能通过 SSH 登录容器手工 export"],
      answer: 0,
      rationale: "Docker bridge 文档列举更优方案：volume 共享文件、compose 定义变量、Swarm secrets/configs 等。",
    },
    {
      id: "w1-4-q23",
      question: "Docker bridge 文档指出：连接到同一 user-defined bridge 网络的容器之间端口可见性通常是？",
      options: ["基本等同于彼此暴露所有端口（无需 publish 也可互访）", "默认全拒绝互访", "必须 publish 才能互访", "只能互访 80/443"],
      answer: 0,
      rationale: "Docker bridge 文档：同一 user-defined bridge 内的容器有效地对彼此暴露所有端口。",
    },
    {
      id: "w1-4-q24",
      question: "Docker bridge 文档指出：要让端口对不同网络的容器或非 Docker 主机可访问，必须？",
      options: ["用 `-p/--publish` 发布端口", "把容器加入默认 bridge", "关闭 iptables", "把网桥 MTU 设为 9000"],
      answer: 0,
      rationale: "Docker bridge 文档：跨网络/对外访问需要 publish ports（-p/--publish）。",
    },
    {
      id: "w1-4-q25",
      question: "Docker bridge 文档指出：默认 published ports 的“默认绑定地址”可通过哪个选项修改？",
      options: ["`com.docker.network.bridge.host_binding_ipv4`", "`com.docker.network.bridge.enable_icc`", "`com.docker.network.driver.mtu`", "`com.docker.network.bridge.enable_ip_masquerade`"],
      answer: 0,
      rationale: "Docker bridge 文档：host_binding_ipv4 选项可修改默认发布端口的绑定地址。",
    },
    {
      id: "w1-4-q26",
      question: "Docker bridge 文档指出：尽管选项名叫 `host_binding_ipv4`，它实际上也可以设置为？",
      options: ["IPv6 地址", "只能是 0.0.0.0", "只能是 127.0.0.1", "只能是容器 IP"],
      answer: 0,
      rationale: "Docker bridge 文档：Despite the option's name, it is possible to specify an IPv6 address。",
    },
    {
      id: "w1-4-q27",
      question: "Docker bridge 文档指出：把默认 binding address 设为 `::` 的效果是？",
      options: ["published ports 只在宿主机 IPv6 地址上可用", "published ports 只在 IPv4 地址上可用", "禁用端口发布", "强制所有端口仅对容器内可见"],
      answer: 0,
      rationale: "Docker bridge 文档：将默认绑定地址设为 :: 时，published ports 仅在宿主 IPv6 地址上可用。",
    },
    {
      id: "w1-4-q28",
      question: "Docker packet-filtering/firewalls 文档指出：若把 daemon 配置中的 iptables/ip6tables 设为 false 且不补充替代规则，可能导致什么结果？",
      options: [
        "bridge 网络容器无法通过 masquerading 访问互联网，但容器端口可能对本地网络主机全部可达",
        "容器仍能正常出网且端口更安全",
        "只影响日志，不影响网络",
        "会自动切换到 host networking 并保持等价行为",
      ],
      answer: 0,
      rationale: "Docker 文档：禁用 firewall 规则会破坏 bridge 网络正确性，容器可能失去出网 masquerading 且端口暴露给本地网络。",
    },
    {
      id: "w1-4-q29",
      question: "Docker packet-filtering/firewalls 文档解释 Docker 与 ufw 不兼容的关键原因是？",
      options: [
        "Docker 把容器流量走 nat 表，published ports 的流量在到达 ufw 使用的 INPUT/OUTPUT 链之前就被改写/分流",
        "ufw 会自动删除 Docker 网络接口",
        "Docker 会禁止 ufw 启动",
        "ufw 只能管理 IPv6 规则",
      ],
      answer: 0,
      rationale: "Docker 文档：published ports 流量会在 nat 表中被路由/改写，可能绕过 ufw 的 INPUT/OUTPUT 规则。",
    },
    {
      id: "w1-4-q30",
      question: "Docker packet-filtering/firewalls 文档对“不要修改 Docker 创建的防火墙规则”的理由是？",
      options: [
        "这些规则用于 bridge 网络隔离、端口发布与过滤，修改可能导致容器网络异常或安全暴露",
        "因为规则都写在容器镜像里，无法修改",
        "因为修改规则会导致 Dockerfile 构建失败",
        "因为规则只影响 Windows 容器",
      ],
      answer: 0,
      rationale: "Docker 文档强调：bridge 网络依赖 Docker 自动创建的防火墙规则实现隔离与端口发布，手工修改易破坏正确性。",
    },
  ],
};