import type { LessonGuide } from "../types"
import type { QuizQuestion } from "@/lib/types"

export const week1Guides: Record<string, LessonGuide> = {
    "ml-w1-1": {
        lessonId: "ml-w1-1",
        background: [
            "【向量与矩阵的本质】线性代数是机器学习的语言。向量表示数据点（如 784 维像素向量代表一张手写数字图片），矩阵表示数据集或线性变换。3Blue1Brown 将矩阵乘法可视化为'空间变换'，这是理解神经网络权重矩阵的关键直觉。",
            "【特征值分解的意义】特征向量是变换的'主方向'——矩阵作用于它们只改变大小不改变方向。特征值表示这个缩放因子。PCA 降维本质是找数据协方差矩阵的特征向量，保留最大特征值对应的方向。",
            "【SVD 的普适性】奇异值分解 (SVD) 可应用于任意矩阵（不限方阵），将矩阵分解为 U·Σ·V^T。在推荐系统、图像压缩、NLP 词向量中广泛应用。低秩近似通过保留前 k 个奇异值实现数据压缩。",
            "【矩阵求逆与伪逆】正规方程求解线性回归需要计算 (X^T·X)^(-1)，但当矩阵不可逆（特征共线性）时失败。Moore-Penrose 伪逆提供最小二乘意义下的解，NumPy 的 np.linalg.pinv 自动处理这种情况。",
            "【张量扩展】深度学习中，数据通常是高维张量：图像是 4D 张量 (batch, height, width, channels)，序列数据是 3D 张量 (batch, sequence, features)。张量运算是矩阵运算的自然推广。"
        ],
        keyDifficulties: [
            "【广播机制陷阱】NumPy 广播允许不同形状数组运算，但容易引入 bug。(n,) 与 (n,1) 行为不同：前者是 1D 数组，后者是列向量。建议始终显式 reshape 确保维度正确。",
            "【数值稳定性】直接计算 (X^T·X)^(-1)·X^T·y 可能因条件数过大而数值不稳定。实践中使用 QR 分解或 SVD：np.linalg.lstsq 内部使用 SVD，更稳定且能处理欠定系统。",
            "【稀疏矩阵】高维特征（如 TF-IDF 文本向量、用户-物品交互矩阵）通常是稀疏的。使用 scipy.sparse 存储可节省 90%+ 内存，但需要选择正确的稀疏格式（CSR 适合行切片，CSC 适合列切片）。",
            "【GPU 加速本质】GPU 并行计算的核心是大规模矩阵乘法。PyTorch/TensorFlow 的张量操作在 GPU 上比 CPU 快 10-100 倍，但数据传输（CPU↔GPU）是瓶颈，需批量处理数据。"
        ],
        handsOnPath: [
            "使用 NumPy 实现矩阵乘法的三种方式：嵌套循环、np.dot、@ 运算符，对比 10000x10000 矩阵的耗时差异（应该是 100 倍以上）。",
            "手动实现 PCA：计算协方差矩阵 → 特征值分解 → 投影到前 k 个特征向量。在 MNIST 数据集上降维到 2D 并可视化，观察数字聚类效果。",
            "使用 SVD 实现图像压缩：对灰度图像矩阵做 SVD，只保留前 k 个奇异值重构图像，观察 k=10, 50, 100 时的压缩效果和信息损失。",
            "实验矩阵求逆的数值问题：构造近似奇异矩阵（条件数 > 10^10），比较 np.linalg.inv 和 np.linalg.pinv 的结果差异。"
        ],
        selfCheck: [
            "解释为什么矩阵乘法 A·B 的复杂度是 O(n³)，而向量化实现比循环快这么多？",
            "PCA 的第一主成分方向有什么几何意义？为什么它能最大化投影后的方差？",
            "SVD 中 U、Σ、V 分别代表什么？低秩近似为什么能实现数据压缩？",
            "什么是矩阵的条件数？条件数过大会导致什么问题？"
        ],
        extensions: [
            "阅读 Gilbert Strang 的 MIT 18.06 线性代数公开课，建立更深的几何直觉。",
            "学习 NumPy 的 einsum 函数，用爱因斯坦求和约定简洁表达复杂张量运算。",
            "探索 JAX 库的自动微分与 JIT 编译，体验现代数值计算框架的性能优化。",
            "研究随机矩阵理论在深度学习初始化（如 Xavier/He 初始化）中的应用。"
        ],
        sourceUrls: [
            "https://www.3blue1brown.com/topics/linear-algebra",
            "https://numpy.org/doc/stable/reference/routines.linalg.html",
            "https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/"
        ]
    },
    "ml-w1-2": {
        lessonId: "ml-w1-2",
        background: [
            "【梯度的几何意义】梯度是多变量函数在某点变化最快的方向，其模是变化率。梯度下降沿负梯度方向移动，因为负梯度指向函数值下降最快的方向。3Blue1Brown 的可视化展示了梯度场如何引导优化过程。",
            "【链式法则的威力】深度学习反向传播的数学基础是链式法则：∂L/∂w = ∂L/∂y · ∂y/∂w。计算图将复杂函数分解为简单操作，反向传播逐层应用链式法则高效计算所有参数的梯度。",
            "【凸优化的重要性】凸函数的局部最小值就是全局最小值，梯度下降保证收敛。线性回归的 MSE 损失是凸的，而神经网络损失面是非凸的——这解释了为什么深度学习优化更困难但往往能找到好的局部解。",
            "【海森矩阵与曲率】海森矩阵是二阶导数矩阵，描述函数的曲率。牛顿法使用 H^(-1)·∇f 作为更新方向，在强凸函数上二次收敛。但计算和存储 H 在高维下不可行，因此衍生出拟牛顿法（BFGS、L-BFGS）。",
            "【鞍点问题】高维非凸优化的主要困难不是局部最小值，而是鞍点（某些方向是极小、某些是极大）。随机梯度下降的噪声有助于逃离鞍点，动量方法也能帮助穿越平坦区域。"
        ],
        keyDifficulties: [
            "【梯度消失/爆炸】深层网络中，梯度经过多次链式法则乘法可能趋近于 0（消失）或无穷大（爆炸）。Sigmoid 的导数最大 0.25，连乘后梯度指数衰减——这是 ReLU 激活函数流行的原因。",
            "【学习率选择】学习率太大导致震荡甚至发散，太小收敛缓慢。自适应学习率方法（Adam、RMSprop）根据历史梯度自动调整，但也引入额外超参数。学习率调度（预热、余弦退火）是常用技巧。",
            "【批量大小与泛化】大批量训练收敛快但泛化差（sharp minima），小批量有正则化效果但训练慢。学术研究表明 flat minima 泛化更好，SGD 的噪声倾向于找到更平坦的解。",
            "【Lipschitz 连续性】梯度的 Lipschitz 常数决定了最大安全学习率。如果 ||∇f(x) - ∇f(y)|| ≤ L||x-y||，则学习率 η < 2/L 保证收敛。这解释了为什么不同层可能需要不同学习率。"
        ],
        handsOnPath: [
            "在 3D 可视化工具中观察 Rosenbrock 函数的梯度下降路径，体验'狭长山谷'中优化的困难，对比普通 GD 与带动量 GD 的轨迹。",
            "手动实现反向传播：构建一个 2 层 MLP 的计算图，用纯 NumPy 计算前向传播和反向传播，验证梯度与数值微分一致。",
            "实验学习率对收敛的影响：在同一任务上使用 lr=0.001, 0.01, 0.1, 1.0 训练，绘制损失曲线对比收敛速度和稳定性。",
            "使用 PyTorch 的 autograd 系统：定义一个复杂表达式，调用 backward() 自动计算梯度，打印计算图结构。"
        ],
        selfCheck: [
            "为什么梯度下降要沿负梯度方向移动？能否沿其他方向？",
            "反向传播的时间复杂度为什么是 O(参数数量) 而不是 O(参数数量²)？",
            "凸函数和非凸函数在优化上的本质区别是什么？",
            "为什么说鞍点比局部最小值更麻烦？高维空间中局部最小值为什么相对少见？"
        ],
        extensions: [
            "阅读 Boyd 的《凸优化》教材，深入理解凸性、对偶性与 KKT 条件。",
            "学习自动微分的两种模式：前向模式 vs 反向模式，理解为什么深度学习用反向模式。",
            "探索二阶优化方法（K-FAC、Shampoo）在大模型训练中的应用。",
            "研究优化器的理论收敛率：SGD 是 O(1/√T)，Adam 在某些条件下可能不收敛。"
        ],
        sourceUrls: [
            "https://www.3blue1brown.com/topics/calculus",
            "https://web.stanford.edu/~boyd/cvxbook/",
            "https://www.ruder.io/optimizing-gradient-descent/"
        ]
    },
    "ml-w1-3": {
        lessonId: "ml-w1-3",
        background: [
            "【概率的两种解释】频率派视概率为长期频率极限，贝叶斯派视概率为信念程度。机器学习同时使用两种观点：MLE（最大似然估计）是频率派，MAP（最大后验估计）引入先验是贝叶斯派。",
            "【贝叶斯定理的核心】P(θ|D) ∝ P(D|θ)·P(θ)——后验正比于似然乘以先验。在机器学习中，θ是模型参数，D是数据。贝叶斯方法自然提供不确定性估计，这在医疗诊断等高风险场景至关重要。",
            "【常见概率分布】高斯分布建模连续变量（身高、误差），伯努利/多项式分布建模离散选择，泊松分布建模计数数据。混合高斯模型能拟合复杂的多峰分布，是聚类算法 GMM 的基础。",
            "【期望与方差】期望 E[X] 是概率加权平均，方差 Var[X] = E[(X-μ)²] 度量分散程度。偏差-方差分解 MSE = Bias² + Variance + Noise 是理解模型泛化的核心框架。",
            "【大数定律与中心极限定理】大数定律保证样本均值收敛到期望，中心极限定理说明独立同分布样本均值趋近正态分布。这是为什么 mini-batch 梯度是总体梯度无偏估计的理论基础。"
        ],
        keyDifficulties: [
            "【条件独立性假设】朴素贝叶斯假设特征条件独立：P(x|y) = ∏P(xᵢ|y)。这个假设几乎总是错的，但算法仍然有效——这是'朴素'的来源。理解何时假设合理、何时会失效很重要。",
            "【共轭先验】某些先验分布与似然函数'共轭'，后验保持相同分布族。如 Beta-Bernoulli、Gaussian-Gaussian。共轭先验使贝叶斯推断有解析解，否则需要 MCMC 近似。",
            "【过拟合的概率解释】过拟合是模型将训练数据噪声当作信号。从贝叶斯角度，过拟合是后验过度集中在某个参数值，正则化等价于加入先验（L2 正则化对应高斯先验）。",
            "【概率校准】分类器输出的'概率'可能未校准——输出 0.8 不一定意味着 80% 的时间正确。Platt scaling 和 isotonic regression 可以后处理校准，在信用评分等场景很重要。"
        ],
        handsOnPath: [
            "用 Seeing Theory 可视化工具交互探索各种概率分布的形状和参数影响。",
            "实现朴素贝叶斯垃圾邮件分类器：计算词频条件概率 P(word|spam)，用贝叶斯定理预测。",
            "模拟中心极限定理：从均匀分布/指数分布抽样，绘制样本均值分布，观察如何趋近正态。",
            "实验贝叶斯参数估计：用 Beta-Bernoulli 模型估计硬币正面概率，观察先验如何影响后验。"
        ],
        selfCheck: [
            "最大似然估计 (MLE) 和最大后验估计 (MAP) 的区别是什么？在什么情况下它们等价？",
            "为什么正态分布在机器学习中如此常见？中心极限定理如何解释这一点？",
            "贝叶斯定理中的'先验'有什么作用？如何选择合适的先验？",
            "什么是概率校准？为什么神经网络的输出概率通常需要校准？"
        ],
        extensions: [
            "学习概率图模型（贝叶斯网络、马尔可夫随机场），理解复杂依赖关系的建模。",
            "探索变分推断和 MCMC，了解如何近似计算复杂后验分布。",
            "阅读 Bishop 的《Pattern Recognition and Machine Learning》第 2 章，深入概率基础。",
            "研究 Bayesian Deep Learning，了解如何为神经网络权重建模不确定性。"
        ],
        sourceUrls: [
            "https://seeing-theory.brown.edu/",
            "https://www.khanacademy.org/math/statistics-probability",
            "https://greenteapress.com/thinkstats/"
        ]
    },
    "ml-w1-4": {
        lessonId: "ml-w1-4",
        background: [
            "【熵的直觉】熵度量不确定性或'惊讶程度'的期望。均匀分布熵最大（最不确定），确定性分布熵为 0。二元熵 H(p) = -p·log(p) - (1-p)·log(1-p) 在 p=0.5 时最大。",
            "【交叉熵的意义】H(P,Q) = -Σ P(x)·log(Q(x)) 度量用分布 Q 编码分布 P 的样本需要的平均比特数。当 Q=P 时达到最小值（即熵）。分类问题中，交叉熵损失促使预测分布 Q 接近真实分布 P。",
            "【KL 散度与信息增益】KL(P||Q) = H(P,Q) - H(P) ≥ 0，度量 Q 相对于 P 的'额外编码代价'。最小化交叉熵等价于最小化 KL 散度。决策树的信息增益就是 KL 散度的应用。",
            "【互信息】I(X;Y) = H(X) - H(X|Y) 度量知道 Y 后对 X 不确定性的减少。在特征选择中，高互信息意味着特征与标签高度相关。互信息在信息瓶颈理论中解释深度学习表征学习。",
            "【从位到奈特】log 底数决定单位：以 2 为底是比特 (bit)，自然对数是奈特 (nat)。机器学习通常用自然对数（因为求导方便），但概念上 bit 更直观。"
        ],
        keyDifficulties: [
            "【数值稳定性】log(0) = -∞ 会导致计算崩溃。实践中使用 np.clip(p, 1e-15, 1) 或 tf.nn.softmax_cross_entropy_with_logits 直接从 logits 计算，避免显式计算 softmax 后再取 log。",
            "【KL 散度不对称】KL(P||Q) ≠ KL(Q||P)。前向 KL 倾向于 Q 覆盖 P 的所有模式（mass-covering），反向 KL 倾向于 Q 集中在 P 的高概率区域（mode-seeking）。VAE 使用反向 KL。",
            "【标签平滑】硬标签（one-hot）的交叉熵对预测过度自信没有惩罚。标签平滑将 [0,0,1,0] 变成 [0.025, 0.025, 0.925, 0.025]，提供正则化效果并改善校准。",
            "【Focal Loss】在类别不平衡时，简单样本主导梯度。Focal Loss 给难样本更高权重：FL(p) = -(1-p)^γ·log(p)，γ>0 时降低易分类样本的损失贡献。"
        ],
        handsOnPath: [
            "计算不同概率分布的熵：均匀分布、偏斜二项分布、正态分布，验证均匀分布熵最大。",
            "实现交叉熵损失函数，比较与 PyTorch 的 F.cross_entropy 结果，注意数值稳定性处理。",
            "可视化 KL 散度的不对称性：画出 KL(P||Q) 和 KL(Q||P) 随 Q 参数变化的曲线。",
            "在 MNIST 分类任务中比较标准交叉熵和带标签平滑的交叉熵的效果。"
        ],
        selfCheck: [
            "熵、交叉熵、KL 散度三者的关系是什么？写出数学公式。",
            "为什么交叉熵是分类问题的默认损失函数？它与极大似然有什么关系？",
            "KL 散度为什么不对称？在哪些场景下选择 KL(P||Q) vs KL(Q||P)？",
            "为什么说最小化交叉熵等价于最小化 KL 散度？"
        ],
        extensions: [
            "阅读 colah 的博客《Visual Information Theory》，获得更深的直觉。",
            "学习信息瓶颈理论，理解深度学习中表征学习的信息论视角。",
            "探索互信息神经估计（MINE）、对比学习中的 InfoNCE 损失。",
            "研究最大熵原理在自然语言处理中的应用（如最大熵分类器）。"
        ],
        sourceUrls: [
            "https://colah.github.io/posts/2015-09-Visual-Information/",
            "https://machinelearningmastery.com/cross-entropy-for-machine-learning/",
            "https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"
        ]
    }
}

export const week1Quizzes: Record<string, QuizQuestion[]> = {
    "ml-w1-1": [
        {
            id: "ml-w1-1-q1",
            question: "PCA 降维的数学本质是什么？",
            options: [
                "找到数据协方差矩阵的特征向量，投影到最大特征值对应的方向",
                "随机选择若干维度进行投影",
                "使用神经网络学习低维表示",
                "对数据进行标准化处理"
            ],
            answer: 0,
            rationale: "PCA 计算数据协方差矩阵的特征值分解，选择最大特征值对应的特征向量作为主成分方向，投影后保留最多方差（信息）。"
        },
        {
            id: "ml-w1-1-q2",
            question: "SVD 分解 A = UΣV^T 中，Σ 矩阵的对角元素是什么？",
            options: [
                "特征值",
                "奇异值（按降序排列的非负实数）",
                "矩阵 A 的行列式",
                "A 的迹"
            ],
            answer: 1,
            rationale: "Σ 是对角矩阵，对角元素是奇异值，按降序排列的非负实数。它们度量每个正交方向上的'强度'。"
        },
        {
            id: "ml-w1-1-q3",
            question: "为什么 NumPy 的向量化操作比 Python 循环快得多？",
            options: [
                "Python 循环有语法错误",
                "向量化操作使用 C/Fortran 实现，避免 Python 解释器开销",
                "向量化操作使用更少内存",
                "向量化操作会跳过某些计算"
            ],
            answer: 1,
            rationale: "NumPy 的向量化操作在底层用 C/Fortran 实现，批量处理数据，避免了 Python 解释器的逐元素循环开销。"
        },
        {
            id: "ml-w1-1-q4",
            question: "矩阵的条件数过大会导致什么问题？",
            options: [
                "计算速度变慢",
                "内存占用增加",
                "数值不稳定，求逆或求解线性方程组结果不准确",
                "无法进行矩阵乘法"
            ],
            answer: 2,
            rationale: "条件数是最大与最小奇异值之比，条件数大意味着矩阵接近奇异，微小的输入误差会被放大，导致数值计算不稳定。"
        },
        {
            id: "ml-w1-1-q5",
            question: "在 NumPy 中，(n,) 和 (n,1) 形状数组的区别是什么？",
            options: [
                "没有区别，只是表示方式不同",
                "(n,) 是 1D 数组，(n,1) 是 2D 列向量，广播行为不同",
                "(n,) 占用更多内存",
                "(n,1) 只能存储整数"
            ],
            answer: 1,
            rationale: "(n,) 是一维数组，(n,1) 是二维列向量。在矩阵运算和广播时行为不同，容易引入维度不匹配的 bug。"
        },
        {
            id: "ml-w1-1-q6",
            question: "为什么线性回归使用 np.linalg.lstsq 比直接计算 (X^T·X)^(-1)·X^T·y 更好？",
            options: [
                "lstsq 计算更快",
                "lstsq 使用 SVD 分解，数值更稳定，能处理奇异矩阵",
                "lstsq 返回更多小数位",
                "lstsq 自动进行特征归一化"
            ],
            answer: 1,
            rationale: "lstsq 内部使用 SVD 分解，即使 X^T·X 接近奇异也能给出合理解，数值稳定性远优于直接求逆。"
        }
    ],
    "ml-w1-2": [
        {
            id: "ml-w1-2-q1",
            question: "梯度下降为什么沿负梯度方向移动？",
            options: [
                "负梯度方向是函数值下降最快的方向",
                "正梯度方向会导致数值溢出",
                "这是随机选择的",
                "负梯度方向能保证全局最优"
            ],
            answer: 0,
            rationale: "梯度指向函数值增加最快的方向，负梯度指向下降最快的方向。沿负梯度走一小步能最大程度减小函数值。"
        },
        {
            id: "ml-w1-2-q2",
            question: "深度网络中梯度消失的主要原因是什么？",
            options: [
                "网络参数太多",
                "Sigmoid 激活函数导数最大 0.25，链式法则连乘后梯度指数衰减",
                "学习率设置太大",
                "批量大小太小"
            ],
            answer: 1,
            rationale: "Sigmoid 的导数 σ'(x) = σ(x)(1-σ(x)) 最大值为 0.25。经过多层链式法则，梯度被反复乘以小于 1 的数，指数衰减到接近 0。"
        },
        {
            id: "ml-w1-2-q3",
            question: "为什么凸优化问题比非凸问题更容易？",
            options: [
                "凸函数计算更快",
                "凸函数的局部最小值就是全局最小值",
                "凸函数不需要计算梯度",
                "凸函数只有一个变量"
            ],
            answer: 1,
            rationale: "凸函数的定义保证了任何局部最小值必然是全局最小值，梯度下降总能找到最优解，无需担心陷入次优局部解。"
        },
        {
            id: "ml-w1-2-q4",
            question: "Adam 优化器结合了哪两种技术？",
            options: [
                "L1 正则化和 L2 正则化",
                "动量和自适应学习率",
                "批量归一化和 Dropout",
                "前向传播和反向传播"
            ],
            answer: 1,
            rationale: "Adam 结合了 Momentum（一阶矩估计，加速收敛）和 RMSprop（二阶矩估计，自适应学习率），对不同参数使用不同有效学习率。"
        },
        {
            id: "ml-w1-2-q5",
            question: "高维非凸优化的主要困难是什么？",
            options: [
                "局部最小值太多",
                "鞍点多，即某些方向是极小、某些方向是极大的点",
                "全局最小值不存在",
                "梯度无法计算"
            ],
            answer: 1,
            rationale: "高维空间中，鞍点的数量远超局部最小值。鞍点处梯度为 0 但不是最优解，SGD 的噪声和动量有助于逃离鞍点。"
        },
        {
            id: "ml-w1-2-q6",
            question: "反向传播的时间复杂度是多少？",
            options: [
                "O(参数数量²)",
                "O(参数数量)",
                "O(层数 × 参数数量)",
                "O(2^参数数量)"
            ],
            answer: 1,
            rationale: "反向传播通过链式法则逐层计算梯度，每个参数只访问一次，总时间复杂度与前向传播相同，是 O(参数数量)。"
        }
    ],
    "ml-w1-3": [
        {
            id: "ml-w1-3-q1",
            question: "贝叶斯定理 P(θ|D) ∝ P(D|θ)·P(θ) 中，P(θ) 代表什么？",
            options: [
                "似然函数",
                "先验分布（对参数的初始信念）",
                "后验分布",
                "数据的边际概率"
            ],
            answer: 1,
            rationale: "P(θ) 是先验分布，表示在观察数据前对参数 θ 的信念。它将领域知识编码到模型中，起正则化作用。"
        },
        {
            id: "ml-w1-3-q2",
            question: "最大似然估计 (MLE) 和最大后验估计 (MAP) 在什么情况下等价？",
            options: [
                "数据量很小时",
                "使用均匀先验（或不使用先验）时",
                "模型是线性的时候",
                "两者永远不等价"
            ],
            answer: 1,
            rationale: "当先验是均匀分布时，P(θ) 是常数，最大化 P(θ|D) 等价于最大化 P(D|θ)，即 MAP 退化为 MLE。"
        },
        {
            id: "ml-w1-3-q3",
            question: "L2 正则化从贝叶斯角度如何理解？",
            options: [
                "对数据加噪声",
                "对权重施加高斯先验",
                "改变似然函数",
                "增加训练数据量"
            ],
            answer: 1,
            rationale: "L2 正则化项 λ||w||² 等价于对权重施加零均值高斯先验 w ~ N(0, 1/λ)。MAP 估计的正则化项来自先验的负对数。"
        },
        {
            id: "ml-w1-3-q4",
            question: "中心极限定理在机器学习中的一个重要应用是什么？",
            options: [
                "保证所有分布都是正态分布",
                "说明 mini-batch 梯度是总体梯度的无偏估计",
                "用于数据增强",
                "决定网络层数"
            ],
            answer: 1,
            rationale: "中心极限定理保证了 iid 样本均值趋近正态分布。mini-batch 梯度是总体梯度的无偏估计，方差随批量大小减小。"
        },
        {
            id: "ml-w1-3-q5",
            question: "朴素贝叶斯的'朴素'体现在哪里？",
            options: [
                "模型参数很少",
                "假设特征在给定类别下条件独立",
                "只能处理二分类",
                "不需要训练数据"
            ],
            answer: 1,
            rationale: "朴素贝叶斯假设 P(x|y) = ∏P(xᵢ|y)，即特征条件独立。这个假设几乎从不成立，但大大简化了计算且效果通常不错。"
        },
        {
            id: "ml-w1-3-q6",
            question: "什么是概率校准？为什么神经网络需要它？",
            options: [
                "让所有预测概率等于 0.5",
                "使输出概率与真实频率一致，神经网络输出通常过度自信",
                "将概率转换为类别标签",
                "标准化输入数据"
            ],
            answer: 1,
            rationale: "校准让预测概率与实际频率一致（预测 80% 的样本确实有 80% 正确）。神经网络往往过度自信，需要温度缩放等方法校准。"
        }
    ],
    "ml-w1-4": [
        {
            id: "ml-w1-4-q1",
            question: "熵 H(X) 在什么分布下达到最大值？",
            options: [
                "正态分布",
                "均匀分布",
                "指数分布",
                "泊松分布"
            ],
            answer: 1,
            rationale: "在所有支撑集相同的分布中，均匀分布熵最大，因为它的不确定性最大，没有任何事件更'可预测'。"
        },
        {
            id: "ml-w1-4-q2",
            question: "交叉熵 H(P,Q) 与 KL 散度 KL(P||Q) 的关系是什么？",
            options: [
                "H(P,Q) = KL(P||Q) - H(P)",
                "H(P,Q) = KL(P||Q) + H(P)",
                "H(P,Q) = KL(P||Q) × H(P)",
                "两者没有关系"
            ],
            answer: 1,
            rationale: "KL(P||Q) = H(P,Q) - H(P)，因此 H(P,Q) = KL(P||Q) + H(P)。最小化交叉熵（H(P) 固定时）等价于最小化 KL 散度。"
        },
        {
            id: "ml-w1-4-q3",
            question: "为什么交叉熵是分类问题的默认损失函数？",
            options: [
                "计算最快",
                "它等价于最大似然估计的负对数似然",
                "它总是非负的",
                "它不需要真实标签"
            ],
            answer: 1,
            rationale: "对于类别分布，最小化交叉熵等价于最大化似然函数的对数（即最大似然估计），是统计学上有良好性质的损失函数。"
        },
        {
            id: "ml-w1-4-q4",
            question: "KL 散度 KL(P||Q) 和 KL(Q||P) 有什么区别？",
            options: [
                "完全相同",
                "前者使 Q 覆盖 P 所有模式，后者使 Q 集中在 P 的高概率区",
                "前者总是更大",
                "只有在离散分布时不同"
            ],
            answer: 1,
            rationale: "KL 散度不对称。前向 KL(P||Q) 惩罚 Q 在 P 高概率处概率低（mass-covering），反向 KL(Q||P) 惩罚 Q 在 P 低概率处概率高（mode-seeking）。"
        },
        {
            id: "ml-w1-4-q5",
            question: "标签平滑（Label Smoothing）的作用是什么？",
            options: [
                "加速训练收敛",
                "防止模型过度自信，提供正则化效果",
                "增加模型参数量",
                "减少内存使用"
            ],
            answer: 1,
            rationale: "标签平滑将 one-hot 标签软化（如 [0,0,1,0] → [0.025, 0.025, 0.925, 0.025]），防止模型对预测过度自信，起正则化作用。"
        },
        {
            id: "ml-w1-4-q6",
            question: "计算交叉熵时 log(0) 的问题如何解决？",
            options: [
                "直接忽略该项",
                "使用 clip 或直接从 logits 计算避免显式 log(softmax)",
                "将 0 替换为 1",
                "改用 MSE 损失"
            ],
            answer: 1,
            rationale: "实践中用 np.clip(p, 1e-15, 1) 或 tf.nn.softmax_cross_entropy_with_logits 直接从 logits 计算，避免 log(0) = -∞ 的数值问题。"
        }
    ]
}
