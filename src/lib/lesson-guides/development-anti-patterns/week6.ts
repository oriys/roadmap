import type { LessonGuide } from "../types"
import type { QuizQuestion } from "@/lib/types"

export const week6Guides: Record<string, LessonGuide> = {
    "ap-6-1": {
        lessonId: "ap-6-1",
        background: [
            "【信任悖论】AI 辅助开发中的默认正确反模式(Default Trust Anti-pattern)：开发者倾向于相信 AI 生成的代码是正确的，减少了必要的审查和验证，导致潜在问题被忽视。",
            "【验证缺失风险】AI 生成的代码可能包含：逻辑错误、安全漏洞、过时的 API 使用、不符合项目规范的代码风格、依赖不安全的第三方库。",
            "【OWASP 代码审查】OWASP Code Review Guide 提供了系统化的代码安全审查方法，对 AI 生成代码同样适用。审查应覆盖身份验证、授权、输入验证、输出编码等方面。",
            "【安全编码指南】OWASP Cheat Sheet Series 提供了各种安全场景的最佳实践。AI 生成代码应该对照这些指南进行验证。",
            "【测试闭环】AI 生成代码必须通过完整的测试流程：单元测试、集成测试、安全测试。AI 本身可以辅助生成测试，但测试覆盖率和测试质量仍需人工审查。"
        ],
        keyDifficulties: [
            "【自动化偏见】心理学上的自动化偏见(Automation Bias)导致人们过度信任自动化系统的输出，在 AI 辅助编程中尤为明显。",
            "【审查疲劳】AI 生成大量代码时，人工审查容易疲劳，导致问题遗漏。需要建立可持续的审查流程和工具辅助。",
            "【上下文缺失】AI 可能不了解项目的完整上下文：架构决策、安全要求、性能约束、团队规范。生成的代码可能'看起来对'但不适合项目。",
            "【责任归属】当 AI 生成的代码导致问题时，责任归属不清。组织需要明确：人类开发者对最终代码负责，AI 是工具而非决策者。"
        ],
        handsOnPath: [
            "建立 AI 代码审查清单：安全性、性能、可维护性、项目规范符合性。",
            "为 AI 生成的代码编写测试：不仅测试功能正确性，还要测试边界条件和异常处理。",
            "使用 SAST（静态应用安全测试）工具扫描 AI 生成的代码。",
            "建立代码审查流程：AI 生成的代码需要与人工编写代码同等甚至更严格的审查。",
            "记录 AI 辅助使用：哪些代码是 AI 生成的，使用了什么提示，进行了哪些修改。"
        ],
        selfCheck: [
            "什么是 AI 辅助开发中的信任悖论？它的危害是什么？",
            "AI 生成的代码可能存在哪些类型的问题？",
            "为什么自动化偏见在 AI 辅助编程中特别危险？",
            "OWASP 代码审查指南的主要审查方面有哪些？",
            "AI 生成代码的测试策略应该包括什么？",
            "当 AI 生成的代码导致问题时，责任应该如何归属？"
        ],
        extensions: [
            "学习 OWASP Code Review Guide 的完整内容。",
            "研究 SAST/DAST 工具如何帮助发现 AI 生成代码中的安全问题。",
            "探索如何在 CI/CD 流程中集成 AI 代码的质量关卡。",
            "了解 AI 代码生成的局限性和已知的失败模式。"
        ],
        sourceUrls: [
            "https://owasp.org/www-project-code-review-guide/",
            "https://cheatsheetseries.owasp.org/",
            "https://cloud.google.com/architecture/ai-ml"
        ]
    },
    "ap-6-2": {
        lessonId: "ap-6-2",
        background: [
            "【影子 AI 定义】Shadow AI 指员工在未经 IT/安全部门批准的情况下使用 AI 工具，类似于影子 IT。这可能导致敏感数据泄露、合规违规和安全风险。",
            "【数据泄露风险】将代码、文档或数据发送给公共 AI 服务可能导致：商业秘密泄露、客户数据违规使用、竞争情报外泄、合规违规（如 GDPR、HIPAA）。",
            "【NIST AI RMF】NIST AI Risk Management Framework 提供了 AI 系统风险管理的系统方法，包括：治理、映射、测量、管理四个核心功能。",
            "【ISO 27001 关联】信息安全管理体系标准 ISO 27001 的原则同样适用于 AI 使用：数据分类、访问控制、供应商管理、事件响应。",
            "【DLP 策略】数据丢失防护(Data Loss Prevention)策略需要扩展到覆盖 AI 工具使用场景，包括检测和阻止敏感数据上传到未授权的 AI 服务。"
        ],
        keyDifficulties: [
            "【便利与安全的矛盾】AI 工具提高效率，员工有动机使用。完全禁止可能导致更隐蔽的使用（真正的影子 AI）。需要在便利和安全间找到平衡。",
            "【数据边界模糊】判断什么数据可以发送给 AI 不总是清晰。代码本身可能不敏感，但可能暴露架构、业务逻辑或安全机制。",
            "【供应商差异】不同 AI 服务的数据处理政策差异大：是否用于训练？数据保留多久？在哪里处理？合规状态如何？",
            "【追踪困难】一旦数据发送给公共 AI 服务，很难追踪其后续使用。即使服务商承诺不使用，技术上难以验证。"
        ],
        handsOnPath: [
            "盘点当前 AI 使用：组织中使用了哪些 AI 工具？谁在使用？处理什么类型的数据？",
            "建立 AI 使用政策：明确哪些 AI 工具被批准、什么数据可以使用、需要什么审批流程。",
            "实施技术控制：网络层阻止未授权 AI 服务、终端检测 AI 工具使用、DLP 检测敏感数据上传。",
            "数据脱敏实践：在使用 AI 前对敏感数据进行脱敏处理，如替换变量名、移除注释中的敏感信息。",
            "定期审计：检查 AI 使用合规性，发现违规及时处理。"
        ],
        selfCheck: [
            "什么是影子 AI？它与影子 IT 有什么相似之处？",
            "使用公共 AI 服务可能导致哪些数据泄露风险？",
            "NIST AI RMF 的四个核心功能是什么？",
            "为什么完全禁止 AI 工具可能适得其反？",
            "DLP 策略如何扩展到覆盖 AI 使用场景？",
            "在使用 AI 前如何对敏感数据进行脱敏？"
        ],
        extensions: [
            "阅读 NIST AI Risk Management Framework 的完整文档。",
            "研究主要 AI 服务商（OpenAI、Anthropic、Google）的数据处理政策。",
            "探索企业级 AI 解决方案（如 Azure OpenAI、AWS Bedrock）的数据隔离机制。",
            "了解行业特定的 AI 合规要求（金融、医疗、政府）。"
        ],
        sourceUrls: [
            "https://www.nist.gov/itl/ai-risk-management-framework",
            "https://www.iso.org/isoiec-27001-information-security.html",
            "https://learn.microsoft.com/en-us/microsoft-365/compliance/data-loss-prevention-policies"
        ]
    },
    "ap-6-3": {
        lessonId: "ap-6-3",
        background: [
            "【提示词注入定义】Prompt Injection 是针对 LLM 应用的攻击，通过在输入中嵌入恶意指令，试图覆盖或绕过系统提示，让模型执行未预期的操作。",
            "【OWASP LLM Top 10】OWASP 发布了 LLM 应用的十大安全风险，提示词注入位列第一，其他包括：不安全的输出处理、训练数据投毒、模型拒绝服务等。",
            "【间接注入】除了直接输入的恶意提示，还有间接注入：恶意内容嵌入在模型处理的文档、网页或数据中，当模型读取这些内容时被'感染'。",
            "【供应链风险】AI 开发依赖大量开源模型、数据集、库。这些依赖可能被投毒或包含漏洞，类似于传统软件的供应链攻击。",
            "【SLSA 框架】Supply-chain Levels for Software Artifacts (SLSA) 框架提供了软件供应链安全的等级标准，可扩展到 AI 模型和数据集的安全管理。"
        ],
        keyDifficulties: [
            "【防御困难】提示词注入难以完全防御，因为 LLM 的工作方式就是处理自然语言输入。传统的输入验证方法效果有限。",
            "【间接注入的隐蔽性】间接注入可能存在于任何模型处理的外部内容中：邮件、文档、网页。用户可能完全不知道自己的请求被'污染'。",
            "【模型透明度不足】很多 AI 模型是黑盒，训练数据和过程不透明。难以评估模型是否被投毒或包含后门。",
            "【快速演进】AI 安全领域变化极快，新的攻击方式和防御措施不断出现。需要持续关注和更新安全实践。"
        ],
        handsOnPath: [
            "测试提示词注入：对你的 LLM 应用进行提示词注入测试，尝试绕过系统提示。",
            "实施输出过滤：对模型输出进行过滤，防止执行危险操作或泄露敏感信息。",
            "审计模型来源：记录使用的模型、版本、来源，建立模型依赖清单（类似 SBOM）。",
            "实施最小权限：LLM 应用只应有完成任务所需的最小权限，不应能访问敏感系统或数据。",
            "监控异常行为：监控 LLM 应用的输入输出，检测可能的注入尝试和异常行为。"
        ],
        selfCheck: [
            "什么是提示词注入？为什么它是 LLM 应用的首要安全风险？",
            "直接注入和间接注入有什么区别？",
            "OWASP LLM Top 10 包含哪些风险类型？",
            "为什么提示词注入难以完全防御？",
            "AI 供应链风险与传统软件供应链风险有什么相似之处？",
            "SLSA 框架如何应用于 AI 模型的安全管理？"
        ],
        extensions: [
            "阅读 OWASP Top 10 for LLM Applications 完整文档。",
            "研究提示词注入的各种攻击技术和防御方法。",
            "探索 AI 模型的安全评估方法和工具。",
            "了解 GitHub Dependabot、Snyk 等工具如何扩展到 AI 依赖管理。"
        ],
        sourceUrls: [
            "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
            "https://slsa.dev/",
            "https://docs.github.com/en/code-security/supply-chain-security"
        ]
    }
}

export const week6Quizzes: Record<string, QuizQuestion[]> = {
    "ap-6-1": [
        {
            id: "ap-6-1-q1",
            question: "什么是 AI 辅助开发中的信任悖论(Default Trust Anti-pattern)？",
            options: [
                "开发者完全不信任 AI 生成的代码",
                "开发者倾向于相信 AI 生成的代码是正确的，减少了必要的审查",
                "AI 不信任开发者的输入",
                "团队成员之间缺乏信任"
            ],
            answer: 1,
            rationale: "信任悖论指开发者倾向于相信 AI 生成的代码是正确的，减少了必要的审查和验证，导致潜在问题被忽视。"
        },
        {
            id: "ap-6-1-q2",
            question: "AI 生成的代码可能存在哪些问题？",
            options: [
                "只有语法错误",
                "逻辑错误、安全漏洞、过时 API、不符合项目规范、不安全的依赖",
                "只有性能问题",
                "只有格式问题"
            ],
            answer: 1,
            rationale: "AI 生成的代码可能包含：逻辑错误、安全漏洞、过时的 API 使用、不符合项目规范的代码风格、依赖不安全的第三方库等多种问题。"
        },
        {
            id: "ap-6-1-q3",
            question: "什么是自动化偏见(Automation Bias)？",
            options: [
                "自动化系统的技术偏差",
                "人们过度信任自动化系统的输出",
                "自动化取代人工的趋势",
                "自动化测试的偏好"
            ],
            answer: 1,
            rationale: "自动化偏见是心理学概念，指人们倾向于过度信任自动化系统的输出，在 AI 辅助编程中尤为明显。"
        },
        {
            id: "ap-6-1-q4",
            question: "OWASP 代码审查应该覆盖哪些方面？",
            options: [
                "只检查代码格式",
                "身份验证、授权、输入验证、输出编码等安全方面",
                "只检查性能",
                "只检查注释"
            ],
            answer: 1,
            rationale: "OWASP Code Review Guide 指出审查应覆盖：身份验证、授权、输入验证、输出编码、错误处理、密码学等安全方面。"
        },
        {
            id: "ap-6-1-q5",
            question: "AI 生成代码的测试策略应该包括什么？",
            options: [
                "只需要单元测试",
                "单元测试、集成测试、安全测试，并审查测试覆盖率和质量",
                "不需要测试，AI 代码是正确的",
                "只需要手动测试"
            ],
            answer: 1,
            rationale: "AI 生成代码必须通过完整测试：单元测试、集成测试、安全测试。测试覆盖率和测试质量仍需人工审查。"
        },
        {
            id: "ap-6-1-q6",
            question: "审查疲劳在 AI 辅助开发中的影响是什么？",
            options: [
                "让审查更高效",
                "AI 生成大量代码时，人工审查容易疲劳，导致问题遗漏",
                "减少审查工作量",
                "提高代码质量"
            ],
            answer: 1,
            rationale: "AI 生成大量代码时，人工审查容易疲劳，导致问题遗漏。需要建立可持续的审查流程和工具辅助。"
        },
        {
            id: "ap-6-1-q7",
            question: "AI 可能不了解项目的哪些上下文？",
            options: [
                "编程语言语法",
                "架构决策、安全要求、性能约束、团队规范",
                "标准库用法",
                "常见算法"
            ],
            answer: 1,
            rationale: "AI 可能不了解项目的完整上下文：架构决策、安全要求、性能约束、团队规范。生成的代码可能'看起来对'但不适合项目。"
        },
        {
            id: "ap-6-1-q8",
            question: "当 AI 生成的代码导致问题时，责任应该如何归属？",
            options: [
                "完全是 AI 的责任",
                "人类开发者对最终代码负责，AI 是工具而非决策者",
                "没有人负责",
                "完全是 AI 供应商的责任"
            ],
            answer: 1,
            rationale: "组织需要明确：人类开发者对最终代码负责，AI 是工具而非决策者。开发者需要审查和验证所有代码。"
        },
        {
            id: "ap-6-1-q9",
            question: "SAST 工具在 AI 生成代码审查中的作用是什么？",
            options: [
                "生成更多代码",
                "自动扫描代码中的安全漏洞和代码质量问题",
                "运行代码",
                "部署代码"
            ],
            answer: 1,
            rationale: "SAST（静态应用安全测试）工具可以自动扫描 AI 生成代码中的安全漏洞、代码质量问题和已知的不良模式。"
        },
        {
            id: "ap-6-1-q10",
            question: "为什么应该记录 AI 辅助使用情况？",
            options: [
                "为了向 AI 收费",
                "追踪哪些代码是 AI 生成的，便于后续审计和问题排查",
                "为了炫耀使用了 AI",
                "没有必要记录"
            ],
            answer: 1,
            rationale: "记录 AI 辅助使用（哪些代码是 AI 生成的、使用的提示、进行的修改）便于后续审计、问题排查和责任追溯。"
        },
        {
            id: "ap-6-1-q11",
            question: "AI 代码审查清单应该包含哪些方面？",
            options: [
                "只检查代码长度",
                "安全性、性能、可维护性、项目规范符合性",
                "只检查变量命名",
                "只检查注释数量"
            ],
            answer: 1,
            rationale: "AI 代码审查清单应覆盖：安全性、性能、可维护性、项目规范符合性等多个方面。"
        },
        {
            id: "ap-6-1-q12",
            question: "AI 生成代码的审查标准应该如何？",
            options: [
                "比人工代码宽松",
                "与人工编写代码同等甚至更严格",
                "不需要审查",
                "只由 AI 自己审查"
            ],
            answer: 1,
            rationale: "AI 生成的代码需要与人工编写代码同等甚至更严格的审查，因为自动化偏见可能导致问题被忽视。"
        }
    ],
    "ap-6-2": [
        {
            id: "ap-6-2-q1",
            question: "什么是影子 AI(Shadow AI)？",
            options: [
                "AI 系统的备份",
                "员工在未经批准的情况下使用 AI 工具",
                "AI 的暗模式界面",
                "AI 的测试环境"
            ],
            answer: 1,
            rationale: "Shadow AI 指员工在未经 IT/安全部门批准的情况下使用 AI 工具，类似于影子 IT，可能导致数据泄露和合规风险。"
        },
        {
            id: "ap-6-2-q2",
            question: "使用公共 AI 服务可能导致哪些数据泄露风险？",
            options: [
                "只有技术风险",
                "商业秘密泄露、客户数据违规使用、竞争情报外泄、合规违规",
                "没有风险",
                "只有性能风险"
            ],
            answer: 1,
            rationale: "将数据发送给公共 AI 服务可能导致：商业秘密泄露、客户数据违规使用、竞争情报外泄、合规违规（如 GDPR、HIPAA）。"
        },
        {
            id: "ap-6-2-q3",
            question: "NIST AI RMF 的四个核心功能是什么？",
            options: [
                "计划、执行、检查、行动",
                "治理、映射、测量、管理",
                "开发、测试、部署、监控",
                "设计、实现、验证、维护"
            ],
            answer: 1,
            rationale: "NIST AI Risk Management Framework 包含四个核心功能：治理(Govern)、映射(Map)、测量(Measure)、管理(Manage)。"
        },
        {
            id: "ap-6-2-q4",
            question: "为什么完全禁止 AI 工具可能适得其反？",
            options: [
                "因为 AI 没有价值",
                "可能导致更隐蔽的使用（真正的影子 AI），需要在便利和安全间找平衡",
                "因为员工不会遵守",
                "因为禁止太困难"
            ],
            answer: 1,
            rationale: "AI 工具提高效率，员工有动机使用。完全禁止可能导致更隐蔽的使用（真正的影子 AI）。需要在便利和安全间找到平衡。"
        },
        {
            id: "ap-6-2-q5",
            question: "DLP 策略如何扩展到覆盖 AI 使用场景？",
            options: [
                "不需要扩展",
                "检测和阻止敏感数据上传到未授权的 AI 服务",
                "只监控 AI 输出",
                "只限制 AI 访问"
            ],
            answer: 1,
            rationale: "DLP 策略需要扩展到覆盖 AI 工具使用场景，包括检测和阻止敏感数据上传到未授权的 AI 服务。"
        },
        {
            id: "ap-6-2-q6",
            question: "判断什么数据可以发送给 AI 的挑战是什么？",
            options: [
                "没有挑战，所有数据都可以发送",
                "代码本身可能不敏感，但可能暴露架构、业务逻辑或安全机制",
                "所有数据都是敏感的",
                "只有文档是敏感的"
            ],
            answer: 1,
            rationale: "判断什么数据可以发送给 AI 不总是清晰。代码本身可能不敏感，但可能暴露架构、业务逻辑或安全机制。"
        },
        {
            id: "ap-6-2-q7",
            question: "数据脱敏在使用 AI 前的作用是什么？",
            options: [
                "让 AI 更高效",
                "移除敏感信息如变量名中的业务术语、注释中的敏感信息",
                "加快处理速度",
                "减少成本"
            ],
            answer: 1,
            rationale: "在使用 AI 前对敏感数据进行脱敏处理，如替换变量名中的业务术语、移除注释中的敏感信息，降低泄露风险。"
        },
        {
            id: "ap-6-2-q8",
            question: "不同 AI 服务商的数据处理政策差异主要体现在哪些方面？",
            options: [
                "只有价格差异",
                "是否用于训练、数据保留时间、处理位置、合规状态",
                "只有功能差异",
                "只有速度差异"
            ],
            answer: 1,
            rationale: "不同 AI 服务的数据处理政策差异大：是否用于训练？数据保留多久？在哪里处理？合规状态如何？需要评估。"
        },
        {
            id: "ap-6-2-q9",
            question: "盘点当前 AI 使用情况时应该了解什么？",
            options: [
                "只了解 AI 工具的名称",
                "使用了哪些 AI 工具、谁在使用、处理什么类型的数据",
                "只了解使用频率",
                "只了解成本"
            ],
            answer: 1,
            rationale: "盘点 AI 使用应了解：组织中使用了哪些 AI 工具？谁在使用？处理什么类型的数据？这是制定政策的基础。"
        },
        {
            id: "ap-6-2-q10",
            question: "企业级 AI 解决方案（如 Azure OpenAI）的优势是什么？",
            options: [
                "更便宜",
                "提供数据隔离机制，数据不用于训练其他模型",
                "功能更多",
                "速度更快"
            ],
            answer: 1,
            rationale: "企业级 AI 解决方案（如 Azure OpenAI、AWS Bedrock）提供数据隔离机制，承诺数据不用于训练其他模型，满足合规要求。"
        },
        {
            id: "ap-6-2-q11",
            question: "ISO 27001 的原则如何应用于 AI 使用？",
            options: [
                "不适用于 AI",
                "数据分类、访问控制、供应商管理、事件响应同样适用",
                "只适用于传统 IT",
                "需要完全新的框架"
            ],
            answer: 1,
            rationale: "ISO 27001 的原则同样适用于 AI 使用：数据分类（什么可以用于 AI）、访问控制（谁可以使用）、供应商管理、事件响应。"
        },
        {
            id: "ap-6-2-q12",
            question: "一旦数据发送给公共 AI 服务后的挑战是什么？",
            options: [
                "数据会自动返回",
                "很难追踪其后续使用，即使服务商承诺不使用也难以验证",
                "可以轻松删除",
                "没有任何挑战"
            ],
            answer: 1,
            rationale: "一旦数据发送给公共 AI 服务，很难追踪其后续使用。即使服务商承诺不使用，技术上也难以验证。"
        }
    ],
    "ap-6-3": [
        {
            id: "ap-6-3-q1",
            question: "什么是提示词注入(Prompt Injection)？",
            options: [
                "一种编程语言",
                "通过在输入中嵌入恶意指令，试图覆盖系统提示让模型执行未预期操作",
                "一种数据库攻击",
                "一种网络攻击"
            ],
            answer: 1,
            rationale: "Prompt Injection 是针对 LLM 应用的攻击，通过在输入中嵌入恶意指令，试图覆盖或绕过系统提示，让模型执行未预期的操作。"
        },
        {
            id: "ap-6-3-q2",
            question: "OWASP LLM Top 10 中排名第一的风险是什么？",
            options: [
                "数据泄露",
                "提示词注入",
                "模型崩溃",
                "网络攻击"
            ],
            answer: 1,
            rationale: "OWASP LLM Top 10 中，提示词注入(Prompt Injection)位列第一，是 LLM 应用面临的首要安全风险。"
        },
        {
            id: "ap-6-3-q3",
            question: "直接注入和间接注入有什么区别？",
            options: [
                "没有区别",
                "直接注入是用户输入恶意提示，间接注入是恶意内容嵌入在模型处理的外部内容中",
                "直接注入更危险",
                "间接注入更容易防御"
            ],
            answer: 1,
            rationale: "直接注入是用户直接输入恶意提示；间接注入是恶意内容嵌入在模型处理的文档、网页或数据中，当模型读取时被'感染'。"
        },
        {
            id: "ap-6-3-q4",
            question: "为什么提示词注入难以完全防御？",
            options: [
                "因为技术不够先进",
                "因为 LLM 的工作方式就是处理自然语言输入，传统输入验证效果有限",
                "因为没有人研究这个问题",
                "因为攻击者太聪明"
            ],
            answer: 1,
            rationale: "提示词注入难以完全防御，因为 LLM 的工作方式就是处理自然语言输入。区分'正常指令'和'恶意指令'本身就是困难的。"
        },
        {
            id: "ap-6-3-q5",
            question: "AI 供应链风险与传统软件供应链风险的相似之处是什么？",
            options: [
                "没有相似之处",
                "依赖的开源模型、数据集、库可能被投毒或包含漏洞",
                "只有成本风险",
                "只有许可证风险"
            ],
            answer: 1,
            rationale: "AI 开发依赖大量开源模型、数据集、库。这些依赖可能被投毒或包含漏洞，类似于传统软件的供应链攻击风险。"
        },
        {
            id: "ap-6-3-q6",
            question: "SLSA 框架是什么？",
            options: [
                "一种编程语言",
                "软件供应链安全的等级标准，可扩展到 AI 模型安全管理",
                "一种数据库",
                "一种 AI 模型"
            ],
            answer: 1,
            rationale: "SLSA（Supply-chain Levels for Software Artifacts）框架提供软件供应链安全的等级标准，其原则可扩展到 AI 模型和数据集的安全管理。"
        },
        {
            id: "ap-6-3-q7",
            question: "间接注入的隐蔽性体现在哪里？",
            options: [
                "攻击代码很小",
                "可能存在于任何模型处理的外部内容中，用户可能完全不知道请求被污染",
                "攻击发生在晚上",
                "攻击者身份隐藏"
            ],
            answer: 1,
            rationale: "间接注入可能存在于任何模型处理的外部内容中：邮件、文档、网页。用户可能完全不知道自己的请求被'污染'。"
        },
        {
            id: "ap-6-3-q8",
            question: "LLM 应用应该遵循什么权限原则？",
            options: [
                "给予最大权限以提高效率",
                "最小权限——只应有完成任务所需的最小权限",
                "与管理员相同的权限",
                "不需要权限控制"
            ],
            answer: 1,
            rationale: "LLM 应用应遵循最小权限原则：只应有完成任务所需的最小权限，不应能访问敏感系统或数据，限制注入攻击的影响范围。"
        },
        {
            id: "ap-6-3-q9",
            question: "输出过滤在防御提示词注入中的作用是什么？",
            options: [
                "让输出更漂亮",
                "防止模型输出执行危险操作或泄露敏感信息",
                "加快响应速度",
                "减少成本"
            ],
            answer: 1,
            rationale: "输出过滤对模型输出进行检查和过滤，防止执行危险操作（如调用系统命令）或泄露敏感信息（如系统提示内容）。"
        },
        {
            id: "ap-6-3-q10",
            question: "模型透明度不足带来的安全挑战是什么？",
            options: [
                "用户体验差",
                "难以评估模型是否被投毒或包含后门，因为很多模型是黑盒",
                "价格不透明",
                "功能不清楚"
            ],
            answer: 1,
            rationale: "很多 AI 模型是黑盒，训练数据和过程不透明。难以评估模型是否被投毒或包含后门，这是供应链安全的重要挑战。"
        },
        {
            id: "ap-6-3-q11",
            question: "类似 SBOM 的模型依赖清单应该记录什么？",
            options: [
                "只记录模型名称",
                "使用的模型、版本、来源，类似软件物料清单",
                "只记录模型大小",
                "只记录模型价格"
            ],
            answer: 1,
            rationale: "应该记录使用的模型、版本、来源，建立模型依赖清单（类似软件物料清单 SBOM），便于追踪漏洞和管理更新。"
        },
        {
            id: "ap-6-3-q12",
            question: "监控 LLM 应用的目的是什么？",
            options: [
                "计算使用量和成本",
                "检测可能的注入尝试和异常行为",
                "提高用户体验",
                "增加功能"
            ],
            answer: 1,
            rationale: "监控 LLM 应用的输入输出，目的是检测可能的注入尝试和异常行为，及时发现和响应安全事件。"
        }
    ]
}
